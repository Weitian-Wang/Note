- [Introduction](#introduction)
	- [Focus of the course](#focus-of-the-course)
	- [Abstract Data Type vs Data Structure](#abstract-data-type-vs-data-structure)
		- [Abstract Data Type](#abstract-data-type)
		- [Data Structure](#data-structure)
		- [Examples](#examples)
- [Analysis of Data Structures](#analysis-of-data-structures)
	- [Worst-case](#worst-case)
	- [Average-case](#average-case)
	- [Amortized Analysis](#amortized-analysis)
		- [Amortized Analysis with Potential Function Method](#amortized-analysis-with-potential-function-method)
- [Array](#array)
	- [Abstract Data Type](#abstract-data-type-1)
		- [Data](#data)
		- [Operations](#operations)
- [Array List - Dynamic Arrays](#array-list---dynamic-arrays)
	- [Abstract Data Type](#abstract-data-type-2)
		- [Data](#data-1)
		- [Operations](#operations-1)
	- [Implementation](#implementation)
		- [Data](#data-2)
		- [Operation \& Amortized Analysis](#operation--amortized-analysis)
			- [Increment](#increment)
			- [Decrement](#decrement)
- [Stack](#stack)
	- [Abstract Data Type](#abstract-data-type-3)
		- [Data](#data-3)
		- [Operations](#operations-2)
	- [Implementation](#implementation-1)
- [FIFO Queue](#fifo-queue)
	- [Abstract Data Type](#abstract-data-type-4)
		- [Data](#data-4)
		- [Operations](#operations-3)
- [Deque - Double-Ended Queue](#deque---double-ended-queue)
	- [Abstract Data Type](#abstract-data-type-5)
		- [Data](#data-5)
		- [Operations](#operations-4)
- [Dictionary Problem](#dictionary-problem)
	- [Abstract Data Type](#abstract-data-type-6)
		- [Data](#data-6)
		- [Operations](#operations-5)
- [Hashing](#hashing)
	- [Basics](#basics)
		- [Load Factor](#load-factor)
		- [Hash Function](#hash-function)
			- [k-independent Hashing](#k-independent-hashing)
				- [Generate k-independent Hash Functions](#generate-k-independent-hash-functions)
				- [Usage](#usage)
			- [Tabulation Hashing](#tabulation-hashing)
				- [Generate Tabulation Hash](#generate-tabulation-hash)
				- [Implementation](#implementation-2)
		- [Rehashing](#rehashing)
	- [Hash Collision](#hash-collision)
		- [Hash Chaining](#hash-chaining)
			- [Operations](#operations-6)
			- [Advantage](#advantage)
			- [Disadvantage](#disadvantage)
		- [Linear Probing](#linear-probing)
			- [Analysis](#analysis)
			- [Code](#code)
			- [Disadvantage](#disadvantage-1)
		- [Quadratic Probing](#quadratic-probing)
		- [Double Hashing](#double-hashing)
		- [Cuckoo Hashing](#cuckoo-hashing)
			- [Visualizing Cuckoo Hashing](#visualizing-cuckoo-hashing)
				- [Directed Graph](#directed-graph)
				- [Undriected Graph](#undriected-graph)
			- [Loop Detection](#loop-detection)
			- [Analysis](#analysis-1)
- [Priority Queue](#priority-queue)
	- [Abstract Data Type](#abstract-data-type-7)
		- [Data](#data-7)
		- [Operation](#operation)
	- [Data Structure](#data-structure-1)
		- [Naive List](#naive-list)
		- [Binary Heap](#binary-heap)
			- [Operations](#operations-7)
			- [Analysis](#analysis-2)
		- [K-ary Heap](#k-ary-heap)
			- [Operations](#operations-8)
		- [Fibonacci Heap](#fibonacci-heap)
			- [Operations](#operations-9)
			- [Analysis](#analysis-3)
				- [Amortized Time of Decrease Priority](#amortized-time-of-decrease-priority)
				- [Amortized Time of Delete Min](#amortized-time-of-delete-min)
	- [Application](#application)
		- [Dijkstra's Algorithm](#dijkstras-algorithm)
			- [Dijkstra's Algorithm with Binary Heap](#dijkstras-algorithm-with-binary-heap)
			- [Dijkstra's Algorithm with K-ary Heap](#dijkstras-algorithm-with-k-ary-heap)
			- [Dijkstra's Algorithm with Fibonacci Heap](#dijkstras-algorithm-with-fibonacci-heap)
		- [Heap Sort](#heap-sort)
- [Set](#set)
	- [Abstract Data Structure](#abstract-data-structure)
		- [Data](#data-8)
		- [Operation](#operation-1)
	- [Implementation](#implementation-3)
		- [Bitmap](#bitmap)
			- [Bitmap Set Operations](#bitmap-set-operations)
			- [Analysis](#analysis-4)
		- [Set in Python - Hash Implementation](#set-in-python---hash-implementation)
			- [Operations](#operations-10)
- [Bloom Filter](#bloom-filter)
	- [Abstract Data Type](#abstract-data-type-8)
		- [Data](#data-9)
		- [Operation](#operation-2)
	- [Application](#application-1)
	- [Analysis](#analysis-5)
		- [Advantages](#advantages)
		- [Disadvantages](#disadvantages)
		- [False Positive Rate](#false-positive-rate)
- [Cuckoo Filter](#cuckoo-filter)
	- [Abstract Data Type](#abstract-data-type-9)
		- [Data](#data-10)
		- [Operation](#operation-3)
		- [Cuckoo Filter vs Cuckoo Hashing](#cuckoo-filter-vs-cuckoo-hashing)
	- [Implementation Details](#implementation-details)
		- [Calculate Positions](#calculate-positions)
			- [Calculate Position for Search](#calculate-position-for-search)
			- [Calculate Second Position From The First Position](#calculate-second-position-from-the-first-position)
			- [Calculate First Position From The Second Position](#calculate-first-position-from-the-second-position)
- [Binary Search Tree](#binary-search-tree)
	- [Introduction](#introduction-1)
	- [Binary Tree](#binary-tree)
		- [Abstract Data Type](#abstract-data-type-10)
			- [Data](#data-11)
		- [Properties](#properties)
			- [Depth](#depth)
			- [Height](#height)
			- [Traversal Orders](#traversal-orders)
			- [Number of Nodes](#number-of-nodes)
	- [Binary Search Tree](#binary-search-tree-1)
		- [Abstract Data Type](#abstract-data-type-11)
			- [Data](#data-12)
			- [Operation](#operation-4)
		- [Analysis](#analysis-6)
			- [Search Time](#search-time)
			- [Delete and Insert](#delete-and-insert)
			- [Analysis of Insertion](#analysis-of-insertion)
		- [Strategies to Minimize Height](#strategies-to-minimize-height)
			- [Rotations](#rotations)
	- [AVL Trees](#avl-trees)
		- [Abstract Data Type](#abstract-data-type-12)
		- [Operation \& Analysis](#operation--analysis)
	- [Red Black Trees](#red-black-trees)
		- [Abstract Data Type](#abstract-data-type-13)
		- [Operation \& Analysis](#operation--analysis-1)
		- [Comparison](#comparison)
	- [Rank-Balanced Trees](#rank-balanced-trees)
		- [Abstract Data Type](#abstract-data-type-14)
		- [Special Cases](#special-cases)
	- [Weak AVL (WAVL) Trees](#weak-avl-wavl-trees)
		- [Abstract Data Type](#abstract-data-type-15)
			- [Data](#data-13)
			- [Operation](#operation-5)
				- [Insertion](#insertion)
				- [Deletion](#deletion)
			- [Analysis](#analysis-7)
- [Special Balanced Trees](#special-balanced-trees)
	- [Recap \& Conclusion of Binary Search Tree](#recap--conclusion-of-binary-search-tree)
	- [Multiway Trees](#multiway-trees)
		- [B-tree or (a, b)-tree](#b-tree-or-a-b-tree)
			- [Abstract Data Type](#abstract-data-type-16)
			- [Operation](#operation-6)
				- [Search](#search)
				- [Insertion](#insertion-1)
				- [Deletion](#deletion-1)
			- [Analysis](#analysis-8)
			- [Application of B-tree](#application-of-b-tree)
	- [Alternatives to Balanced Trees with Randomization](#alternatives-to-balanced-trees-with-randomization)
	- [Treaps](#treaps)
		- [Abstract Data Type](#abstract-data-type-17)
			- [Data](#data-14)
			- [Operation](#operation-7)
				- [Insertion](#insertion-2)
				- [Deletion](#deletion-2)
		- [Analysis](#analysis-9)
	- [Skip Lists](#skip-lists)
		- [Abstract Data Type](#abstract-data-type-18)
		- [Analysis](#analysis-10)
	- [Zip Tree](#zip-tree)
		- [Abstract Data Type](#abstract-data-type-19)
			- [Data](#data-15)
			- [Operation](#operation-8)
				- [Insertion](#insertion-3)
				- [Unzipping](#unzipping)
				- [Deletion](#deletion-3)
				- [Zipping](#zipping)
		- [Analysis](#analysis-11)
			- [Zip Tree and Skip List](#zip-tree-and-skip-list)
			- [Zip Tree and Treap](#zip-tree-and-treap)
	- [Alternatives that Improve Amortized Performance](#alternatives-that-improve-amortized-performance)
	- [Splay Trees](#splay-trees)
		- [Abstract Data Type](#abstract-data-type-20)
		- [Operation](#operation-9)
			- [Case 1 zig](#case-1-zig)
			- [Case 2 zig-zig](#case-2-zig-zig)
			- [Case 3 zig-zag](#case-3-zig-zag)
			- [Search](#search-1)
			- [Insert](#insert)
			- [Delete](#delete)
		- [Analysis](#analysis-12)
- [String Matching](#string-matching)
	- [Dictionary Problem](#dictionary-problem-1)
	- [Prefix Tree](#prefix-tree)
		- [Abstract Data Type](#abstract-data-type-21)
		- [Operation](#operation-10)
		- [Analysis](#analysis-13)
	- [Trie](#trie)
		- [Abstract Data Type](#abstract-data-type-22)
		- [Operation](#operation-11)
		- [Analysis](#analysis-14)
	- [Compressed Trie](#compressed-trie)
		- [Abstract Data Type](#abstract-data-type-23)
	- [Suffix Tree](#suffix-tree)
		- [Operation](#operation-12)
	- [Compressed Suffix Tree](#compressed-suffix-tree)
	- [Compacted Suffix Tree](#compacted-suffix-tree)
		- [Abstract Data Type](#abstract-data-type-24)
	- [Compressed vs Compacted](#compressed-vs-compacted)
- [Range Queries](#range-queries)
	- [Decomposable Range Queries](#decomposable-range-queries)
	- [Dynamic 1-Dimentional Decomposable Range Queries](#dynamic-1-dimentional-decomposable-range-queries)


# Introduction
## Focus of the course
Anlyze the performance of the algorithm implemented with given data structure.  
## Abstract Data Type vs Data Structure
### Abstract Data Type
Abstract data type defines the **logical** form of the data type. We care about the data type and operations about ADT.
### Data Structure
The data structure implements the **physical** form of the data type.  
### Examples
Dictionary
Abstract Data Type:  
- Data type: key-value pairs  
- Operations: 
  - Query: find value associated with a given key
  - Update: store value of a given key  

Data Structure:  
The actual implementation of the ADT above:  
- Hashing schemes
- Balanced binary search trees

# Analysis of Data Structures
## Worst-case
Restrictive time used for real-time response time analysis.
## Average-case
Expected time value, taking input probilities in to account. As we are making assumption about the probilities, average-case time can be inaccurate.
## Amortized Analysis
**Worst** case time for a **sequence** of operations.  
$$\text{Total Actual Time} \leq \text{Total Amortized Time}$$
Worst-case amortized time is the upper bound of worst-case actual time. [Proof see note 2-15](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes2-handout.pdf).  
### Amortized Analysis with Potential Function Method
1. Define a non-negative potential function $\Phi$, describing the states of the data structure. Initially $\Phi=0$
> Think potential function as the distance between current state of the data structure and the idea state. One analogy would be the gravitational potential energe.  

2. Define amortized time of an operation:
$$\text{amortized time} = \text{actual time} + C\times(\Phi_{new}-\Phi_{old})$$
> The amortized time equals actual time plus change of potential. If change of potential is positive, the data structure is further from its idea state, which is going to cost more time for future operations.  

> The potential is 0 when the data structure is initialized.  
> The potential of the data structure is non-negative(zero or positive).    
> The change of potential $\Delta\Phi=\Phi_{new}-\Phi{old}$ may be negative.
> The actual time of an operation is always positive.
> The amortized time of an operation may be negative, which will make it O(1).

# Array
## Abstract Data Type
### Data
Contiguous store of data items, with fixed length
### Operations
1. Create array of length n, 0-indexed by convention
2. Store a given value at a given index
3. Retrieve value stored at a given idex


# Array List - Dynamic Arrays
## Abstract Data Type
### Data
Continuous store of data items, with variable length. Combines the functionalities of arrays and linked lists.  
### Operations
1. Create a new ArrayList of length n
2. Return length of the current ArrayList
3. Store an item in given location index i
4. Reterieve the item stored in location index i
5. Increase the current length by 1
6. Decrease the current length by 1
## Implementation
Use fixed length array to implement ArrayList
### Data
Current number of elements (or length) L.  
Underlying array B, with $|B|\geq L$.
> $|B|$ denotes the length of array B.  

Keep $|B|/4 \leq L \leq |B|$, if smaller than $|B|/4$ shrink size to idea size, if larger than $|B|$ expand size to idea size.     
Ideal state $L = |B|/2$
### Operation & Amortized Analysis
Given that the ideal state of this data structure implementation is $L = |B|/2$, we can define the potential function
$$\Phi=|2L-|B||$$
#### Increment
Increase the length of ArrayList by 1, if L is too big that it exceeds $|B|$, reallocate underlying array.
```
L = L + 1
if L > sizeof(B):
  B_NEW = allocate new array of size 2*L
  copy B in to B_NEW
  set remaining location of B_NEW to null
  B = B_NEW
```
Amortized time for increment operation where B is resized:  
$\text{Potential Fucntion} \Phi=|2L-|B||$  
$\text{Amortized Time} = \text{Actual Time} + C\times \Delta\Phi$  
$\text{Actual Time} \leq c\times(L+1)$ for copying $L+1$ elements.  
$\Phi_{old}=|2L-|L||=L$  
$\Phi_{new}=|2L-|2L||=0$  
$\text{Amortized Time} \leq c\times(L+1) + c\times(\Phi_{new}-\Phi_{old})$  
$=c\times(L+1)+c\times(-L)=c=O(1)$  
#### Decrement
Decrease the length of ArrayList by 1, if L is smaller than $|B|/4$, resize the underlying array to $2L$.  
Amortized time for decrement operation where B is resized:  
$\text{Potential Fucntion} \Phi=|2L-|B||$  
$\text{Amortized Time} = \text{Actual Time} + C\times \Delta\Phi$  
$\text{Actual Time} \leq c\times(L-1)$ for copying $L-1$ elements.  
$\Phi_{old}=|2L-|4L||=2L$  
$\Phi_{new}=|2L-|2L||=0$  
$\text{Amortized Time} \leq c\times(L+1) + c\times(\Phi_{new}-\Phi_{old})$  
$=c\times(L+1)+c\times(0-2L)=-cL=O(1)$  
> As per lecture note, negative amortized time is valid, and quantity would be $O(1)$
```
L = L - 1
if 4*L < sizeof(B):
  B_NEW = allocate new array of size 2*L
  copy data, set other locations
  B = B_NEW
```

# Stack
## Abstract Data Type
### Data
Contiguous store of data items, with variable length and FILO/LIFO properties.
### Operations
1. Create an empty stack
2. Push: insert an item at the top of the stack
3. Pop: remove the item at the top of the stack
## Implementation
Stack can be implemented with ArrayList. When using implementing something with known data structure, we can use its amortized operation times as new base operation times.

# FIFO Queue
## Abstract Data Type
### Data
Contiguous store of data items, with variable length and FIFO properties.
### Operations
1. Create an empty queue
2. Enqueue: insert an item at the rear of the queue
3. Dequeue: Remove the item at the front of the queue

# Deque - Double-Ended Queue
## Abstract Data Type
### Data
Contiguous store of data items, with combined functionality of stack and queue.
### Operations
1. Create and empty deque
2. Insert an item at the front of the deque
3. Insert an item at the rear of the deque
4. Remove an item at the front of the deque
5. Remove an item at the rear of the deque

# Dictionary Problem
## Abstract Data Type
### Data
Collection of key-value pairs.  
Keys can be numbers, string, memory address, etc.  
Values can be basic data types or references.
### Operations
1. Search value associated with given key
2. Update value of given key, or add new pair if key not previously exist in the dictionary
3. Delete pair associated with given key

# Hashing
Suppose we have n key-value pairs, n may change as we perform set/delete operations.  
Maintain a hash table H of size N > n, N may require resizing as n changes.
## Basics
### Load Factor
Load factor is the number of elements in a hash table divided by the total number of table slots.  
$$\alpha=\frac{n}{N}$$
### Hash Function
For $K$ possible keys to $N$ possible index values.  
Hash function h: $\text{keys}\rarr\text{indices in hash table}$  
#### k-independent Hashing
Definition of k-independent: for every $k$-tuple of keys, all $N^k$ $k$-tuples of indice values are equally likely.
##### Generate k-independent Hash Functions
1. Choose a prime $p\gg N$
2. Randomly choose k numbers $a_0, a_1, \dots, a_{k-1}$ in range $[0, p-1]$
3. User polynomial to generate hash value $h(x)=((a_0+a_1\cdot x+a_2\cdot x^2+\dots+a_{k-1}\cdot x^{k-1})mod\ p)mod\ N$
##### Usage
- Hash chaining: 2-independence for O(1) expected time
- Linear probing: 5-independence
- Cuckoo hashing: $O(\log n)$-independence
#### Tabulation Hashing
##### Generate Tabulation Hash
Assume keys are 32-bit integers -> 4 byte -> 4 8-bit segmentations
1. Preprocessing: Build four tables $T_i[\cdot]$ of 256 random numbers each.
2. h(k):
    - Partition k in to 4 bytes: $k_0, k_1, k_2, k_3$
    - Return $T_0(k_0)\oplus T_1(k_1)\oplus T_2(k_2)\oplus T_3(k_3)$
> $\oplus$ bitwise XOR
##### Implementation
Above tabulation hash function is 3-independent.
### Rehashing
If we need to resize H to accomodate more elements, we need choose a new hash function that map all possible key to new N indices.
## Hash Collision
Different hash key mapped to the same hash index value.  
Few strategies discussed in the lecture for dealing with hash collsions:
### Hash Chaining
Each cell of hash table H store a collection (as ArrayList or linked list) of key-value pairs.
#### Operations
Estimated time per operation = $O(1+\alpha)$.
> 1 comes from the inevitable hash opertion and collection lookup  
> The number of n-1 existing keys colliding with k is $(n-1)\times\frac{1}{N}$, assuming hash function generate N indices with the same likelihood.  
1. Search(key): first find the collection in the hash table, then scan through the collection, looking for the pair corresponds to given key.
2. Set(key, value): Update pair or add new pair to the collection.
3. Delete(key): delete the pair in the collection.
#### Advantage
Hash chaining is a simple solution that works.  
#### Disadvantage
Extra space for storage and slower access time.
### Linear Probing
Each cell of hash table H store one key-value pair.  
Try to store (k,v) in index position h(k), if full try h(k)+1, h(k)+2, so forth and wrap around modulo N.
#### Analysis
Omit the proof, the expect search time of hash table with linear probing is $O(1)$.  
Expected time for successful search
$$O\left(1+\frac{1}{(1-\alpha)}\right)$$
Expected time for unsuccessful search
$$O\left(1+\frac{1}{(1-\alpha)^2}\right)$$
#### Code
```
def search(k):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  // key == k
  if H[i] is non-empty:
    return H[i].value
  else:
    exception
```
```
def set(k,v):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  H[i] = v
```
A simple solution for deletion would be mark the deleted position with a flag, indicating the value is nolonger available.  
The following code is for moving probed elements to the front.
```
def delete(k):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  if H[i] is empty:
    exception
  j = (i+1)%N
  while H[j] is non-empty:
    if h(H[j].key) not in circular range [i+1, j]:
      move H[j] to H[i]
      i = j
    j = j + 1
```
i points to available empty spot in the left side  
j scan through current chunck of elements      
#### Disadvantage
Performance degrades as load factor $\alpha=\frac{n}{N}$ gets close to 1.
### Quadratic Probing
Instead of looking for next one slot, look for $h(k)+1$, $h(k)+4$, $h(k)+9$, etc.
### Double Hashing
User a secondary hash function $h_2(k)$, try $h(k)+h_2(k)$, $h(k)+2\times h_2(k)$, $h(k)+3\times h_2(k)$, etc. 
### Cuckoo Hashing
Two hash tables: $H_0$, $H_1$  
Two hash functions: $h_0$, $h_1$  
Search(k): Look in both places $H_0[h_0(k)]$, $H_1[h_1(k)]$  
Set(k,v): Start from table zero, insert at $H_0[h_0(k)]$, if pair $(k', v')$ is already there, evict it to $H_1[h_1(k')]$ and so forth.  
#### Visualizing Cuckoo Hashing
We visualize cells as vertices, keys as edges. 
##### Directed Graph
Direct each edge **towards** the cell that cintains the key.  
A graph is a valid state only if each vertex (cell) has at most 1 incoming edge.
##### Undriected Graph
If two vertices have three **paths**, there is no way to orient the edges so that each vertex has only one incoming edge.
#### Loop Detection
1. Loop time threshold: project 1 implementation
2. Explicitly check for cycle: homework 1 track the starting key, if we try to place it three times ($t_0$, $t_1$, $t_0$ again) we are in the loop.
#### Analysis
Guranteed $O(1)$ worst-case search time, at the cost of slower set operation.  
For a sequence of n operations, the expect number of rebuild is 1, rebuild takes $O(n)$, so expect time **per operation** is O(1).

# Priority Queue
## Abstract Data Type
API
### Data
A set of items with associated priorities. By convention the smaller numbers have the higher priorities.
### Operation
1. Create priority queue for a set of items
2. Add and remove items
3. Find the item with minimum value
4. Change the priority of an item
## Data Structure
Physical implementations of a priority queue can be lists of heaps.
### Naive List
Use unsorted lists or sorted lists to implement priority queues.  
Homework 2 represent the queue as a dynamic array of data values.  
### Binary Heap
An **array** $A$ of $n$ values interpreted as a complete binary tree.  
1. Root = $A[0]$  
2. Children($A[i]$)=$(A[2i+1],\ A[2i+2])$  
3. Parent($A[i]$)=$A[floor\left ( \frac{i-1}{2}\right )]$  
4. Height = $\log_2 (n)$
#### Operations
1. Find min: return $A[0]$
2. Sift down at index i: if $A[i]$ is larger than its descendants, move it down the tree. Worst-case $O(\log n)$.  
    ```
    def siftDown(i):
        // two comparison per iteration
        while A[i] larger than any of two children:
            swap A[i] with best child
            // update i continue sift down until smaller than both children
            i = index of best child
    ```
3. Sift up at index i: if $A[i]$ is smaller than its parent, move it up the tree. Worst-case $O(\log n)$.  
    ```
    def siftUp(i):
        // one comparison per iteration 
        while A[i] smaller than parent:
            swap A[i] with parent
            // update i continue sift up until ;arger than parent
            i = index of best parent
    ```
4. Delete item at given index $i$: move last element $A[n-1]$ to $A[i]$, then do both sift_up(i) and sift_down(i)
5. Delete min would be delete item at index 0
6. Make heap from an array A of n items
    ```
    def makeHeap():
        for i in n-1, n-2, ..., 1, 0:
            siftDown(i)
    ```
    > Makeheap analysis see [note 5-11](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes5-handout.pdf).  
    > Why we run siftDown on each position for creation rather than siftUp? Operation siftDown is costy for leaves heigher up in the tree, siftUp is costy for leaves closer to the bottom. There are more nodes closer to the bottom in a complete binary tree. Therefore siftDown is the faster choice.
6. Insert item `x`, append `x` as the last node in heap, then sift_up.  

#### Analysis
Make heap from a set of n items $O(n)$ time, all other operations have $O(\log n)$ time.
### K-ary Heap
Instead of two children, each node in a k-ary heap have k children.  
Suppose we have k-ary tree with n nodes.  
Height of tree: $\log_k n = \frac{\log k}{\log n}$  
Children($A[i]$) = $(A[ki+1],\ A[ki+2],\ A[ki+3], \dots, A[ki+k])$  
Parent($A[i]$) = $A[floor\left ( \frac{i-1}{k}\right )]$  
#### Operations
1. Sift up is the same as binary heap, do 1 comparison per level
2. Sift down: we need to find the minimum amongst all k children. Which makes it $O(k\cdot\text{height})$
### Fibonacci Heap
**Forest** of heap-ordered trees with pointer to the best tree root.  
Each node has:  
1. Data item and priority value
2. Pointer to parent node
3. Number of children
4. Pointer to a child node
5. Doubly-link pointers to left and right neighbors.
6. A Boolean flag, True when
    - node is not root
    - has one child removed  

Potential function $\Phi = (\text{number of root nodes} + 2 \cdot(\text{number of True flags}))$  

#### Operations
1. Find min: return item pointed be the min pointer, which points to the minimum value amongst all root nodes. $\Delta\Phi=0$.
2. Create new Fibonacci Heap from n items: create a root node for each item, link items. $\Delta\Phi=n$ as we created n more root nodes.
3. Insert item: create root node for the new item, update min pointer. $\Delta\Phi=1$ as we created 1 more root node.
4. Merge **two** Fibonacci Heaps: Concatenate root lists and update best root pointer. $\Delta\Phi=0$.  
> We are doing operation in the laziest fashion so far. The strategy is to delay the work until we can no longer ignore them.  

5. Decrease priority of target node x:
    - If x is root, decrease priority inplace
    - If x is not a root, make it x a root then decrease
        - If x's parent p is not root, and flag is False, set to True
        - If x's parent p is not root, and flag is True, make p a root, and so forth for p's parent.
    > Remove subtree from one tree may cause a chain reaction of node updates or promotions (become root node).  
  
    > Meaning of the boolean flag is to track when a non-root node loses its second child, it has to become a root.

6. Delete minimum: delete the root where minimum value resides, children of deleted root become roots and set the flags as False. Then improve the forest.
7. Improve: force all **root nodes** to have different numbers of children (at most $M=\log n$).
      ```
      // use array C to track roots' numbers of children
      // initially C is empty and R contains all roots
      // allocate array C of size M+1, let R be the collection not yet in C
      while R is nonempty:
          remove x from R
          // no children number conflict
          if C[x.childCount] == null:
              C[x.childCount] = x
          else:
              // set x as children of existing root
              combine x, C[x.childCount] to a new tree y
              // add new tree to working collection
              add y to R
              // remove existing root with same number of children as x
              C[x.childCount] == null
      ```
#### Analysis
##### Amortized Time of Decrease Priority
Let k be the number of promotions triggered by one decrease priority operation.  
Actual time: $O(k+1)$ for k promotions and decrease priority.  
With k promotions we created k new roots.   
With k promotions we also flipped k-1 (or k if taget node has True flag) flags from True to False.  
Change of potential $\leq k-2(k-1)+2=-k+O(1)$. Worst case amortized time = $O(1)$.
##### Amortized Time of Delete Min
Overall amortized time $O(M)$, where $M$ is the maximum number of children that a node is allowed to have. We choose $M=\log n$ to get $O(\log n)$ as amortized time for DeleteMin.
## Application
### Dijkstra's Algorithm
The given graph contains n vertices and m edges.  
Initialize a priority queue Q of vertices.  
```
// iterate n times
while Q is nonempty:
   find and remove minimum vertex v
   // iterate m times in total, not in one loop
   for each edge from v -> w:
      // update priority
      D[w] = min(D[w], D[v] + length(v->w))
```
#### Dijkstra's Algorithm with Binary Heap
Delete minimum is executed n times $O(n\log n)$.  
Updata priority is executed m time $O(m\log n)$.  
> Use additional dictionary to track the nodes so we don't need to search for the nodes in the array before changing priorities.  
  
> As for Dijkstra's algo we are only decreasing the priorities of nodes, therefore **sift up** would be sufficient to fix heap after change priorities or delete min.  

Total runtime of Dijkstra's algorithm with binary heap $O(n\log n + m\log n)$. For a connected graph m >= n - 1, which overshadows n. Hense time $O(m\log n)$. EXPLANATION NOT VARIFIED  

#### Dijkstra's Algorithm with K-ary Heap
Recap the level of k-ary tree with n nodes = $\log_k n = \frac{\log k}{\log n}$  

Delete min operation now cost $O(k\cdot\frac{\log n}{\log k})=O(\frac{k}{\log k}\cdot\log n)$, which is **slower** than binary heap's $O(\log n)$.  

Decrease priority operation cost one conparison each level, but with k-ary tree there are fewer levels. $O(1\cdot \text{depth of tree}) = O(\frac{1}{\log k}\cdot\log n)$, which is **faster** than than binary heap's $O(\log n)$.  

The total time $O(n\cdot\text{deleteMin}+m\cdot\text{decreasePriority})$ = $O(n\cdot\frac{k}{\log k}\cdot \log n + m\cdot\frac{1}{\log k}\cdot \log n)$.  

Proof ommited. Choose $k=\frac{m}{n}$ and when $m=n^c,\ c > 1$, the complexity simplifies to $O(m)$.

> Interpretation: for graph with number of edges that is exponential to the number of vertices, Dijkstra's algo with k-ary heap runs at $O(m)$ time.

#### Dijkstra's Algorithm with Fibonacci Heap
Recap decrease priority $O(1)$ and delete min $O(\log n)$.  
Total time for Dijkstra's algorithm with Fibonaccia Heap = $O(m+n\log n)$
### Heap Sort
Implement priority queue as a heap for a selection sort algorithm. 

# Set
## Abstract Data Structure
### Data
Each item is included only once without repetitions. For Python, only hashable items are allowed to be stored in a set.  
### Operation
1. Construct a set from a list of items
2. Union of two sets
3. Intersection of two sets
4. Compare two sets
5. Check if item in set
6. Add item to set
7. Remove item from set

## Implementation
### Bitmap
When the elements are non-negative integers `[0, 1, 2, ..., n-1]` total of `n` elements or can be numbered using small integers. We can use n-bits to represent the n-elements.  
The n-bits binary vector can be interpreted as a integer number.  
For example the set of `{0, 3, 6}` where elements `x = 6`, `y = 3` and `z = 0`, can be represented as following binary vector.  
```
bit  7 6 5 4 3 2 1 0

val  0 1 0 0 1 0 0 1
       ▲     ▲     ▲
ele    x     y     z
``` 
Integer value of the binary vector = $2^x + 2^y + 2^z$.
#### Bitmap Set Operations
1. Create `{x, y, z}`, `(1<<x)|(1<<y)|(1<<z)`, left shift 1 bit multiple times according to the element value, then bit-or the results.
2. Union of set `S` and `T`, `S|T`
3. Intersection of `S` and `T`, `S&T`
4. Check if item `x` in set `S`, `(1<<x)&S`, check if x-th bit (counting from **right to left**) is 1
5. Add item `x` to set `S`, `S = (1<<x)|S`
6. Remove item `x` from set `S`, `S = S&~(1<<x)`, set x-th bit to 0
#### Analysis
Pros: Bitmap implementation of map is fast if the size of all possible elements is smaller than word length (which usually corresponds to the integer size). Constant time $O(1)$ for all operation in this case.  
Cons: If universe of possible set elements exceeds word size, we need to distribute the universe into multiple words, add to the operation time. $O(\text{universe size}/\text{word size}+1)$. First time for word lookup, then constant time for operation within the word.  

### Set in Python - Hash Implementation
We can treat set as **hash** table with only keys.
#### Operations
1. Add item and remove item, `set.add` & `set.remove`, $O(1)$ expected time.
2. Build a set of `n` elements, equivalent to add `n` times, $O(n)$ 
3. Union of `S` and `T`, $O(n)$ as we need to traverse both sets
4. Intersection of `S` and `T`, $O(n)$ as we need to traverse at least the smaller of two sets
6. Check if item in set, $O(1)$

# Bloom Filter
Fast but inexact representation of large sets.  
Possible false positive but no false negative.  
> False positive: the item is not in the set but test report true.  

If item $x\in S$, always report true.  
If item $x\notin S$, may still report true.  
```
 Gnd\Rst  True    False

 True     TP      FN

 False    FP      TN
```
## Abstract Data Type
### Data
A combination of hash an bitmap.  
A set `S` of `n` values to be represented.  
Parameter `k`, a small integer, each value are hashed to a `k`-tuple.  
Table `H` of size `N` bits, `N > kn`, larger than possible number of items time k-bit representation per item.  
### Operation
1. Initially set all bits of `H` to 0
2. To add item `x`, hash `x` get k-tuple, set all corresponding k-bits to 1
3. To check if item `x` in set, hash `x` get k-tuple, check if every k-bits are 1s
4. Union of `S` and `T`, bit-or of two sets

## Application
1. Allow access with allow-list, prevent denial of service attack with firewall.  
2. Deny access with block-list, allow most of the legitimate traffic.  

## Analysis
### Advantages  
1. Fast even on large sets  
2. Supports adds and unions
3. We don't need to know the universe of possible items because of using hash, unlike bitmap.

### Disadvantages
1. Inexact as there might be false positive reports
2. No remove and intersection support

### False Positive Rate
Fixed value number of bits representing each item `k`, bit table size `N`.  
Let `p` be the probability that one bit is 1.  
For a key `x` thats not in the set, the probability of false positive is $p^k$, which is the probability all `k` bits representing `x` is 1.  


# Cuckoo Filter
Fast but inexact representation of large sets.  
The implementatio is similar to Cuckoo Hashing.  
## Abstract Data Type
### Data
A key can be stored in one of **two locations**, based on the hash values of two hash functions $h_0$ and $h_1$.  
We could use a single table `H`, or two tables $H_0$ and $H_1$.  

### Operation 
1. Search for key `x`: look in both possible locations, may have false positive results  
2. Insert key `x`: Store fingerprint of `fp(x)` the first location, if bucket is full (already has `k` fingerprints), **randomly** choose one of the `k` and move it to its other position and so forth  
3. Delete key `x`: look for fingerprint of the key in both locations, if `fp(x)` is found in either location, remove an instance of `fp(x)`. Can cause incorrect deletion if item `x` is not previously added  


### Cuckoo Filter vs Cuckoo Hashing
Cuckoo Filter is a set implementation  
1. store only the fingerprint `fp(x)` of the key
2. each position of the table can contain `k` fingerprints, **multiple** data in each position
3. can be implemented using one table or two tables
4. fingerprint is stored we need a new technique to calculate its other position  

Cuckoo Hashing is a hash table implementation  
1. store the entire key itself in the table
2. each position of the table can store only one key
3. needs to be implemented with two tables
4. since the original key is stored, we can calculate its other position with ease

## Implementation Details
### Calculate Positions
As mentioned above, since we are storing the **fingerprints** of keys instead of the keys, we **can not** know the other position of a fingerprint `fp(x)` just by applying the other hash function.   
Solution is to exploit the reversible property of the XOR $\oplus$  
Two hash functions $h_0$ and $h_1$, fingerprint of key `k` $fp(k)$.  
1. First location $h_0(k)$, fingerprint stored $fp(k)$  
2. Second location $h_0(k)\oplus h_1(fp(k))$, data stored also $fp(k)$  
3. For a simplified solution we can dump the second hash function $h_1$ all together  
#### Calculate Position for Search 
Known original key $k$, $h_0$, $h_1$ and $fp$ when start searching.  
Location 1 = $h_0(k)$  
Location 2 = $h_0(k)\oplus h_1(fp(k))$  
#### Calculate Second Position From The First Position
During the insertion move around process, we don't know the key of the fingerprints we encountered.  
Known $h_0$, $h_1$, fingerprint $fp(k)$ stored in the first position $h_0(k)$.  
Second location = $h_0(k)\oplus h_1(fp(k))$  
#### Calculate First Position From The Second Position
Known $h_0$, $h_1$, fingerprint $fp(k)$ stored in the first position $h_0(k)\oplus h_1(fp(k))$.  
Second location = $h_0(k)\oplus h_1(fp(k)) \oplus h_1(fp(k)) = h_0(k)$.  
Conclusion XOR current location with hash_1 of stored fingerprint to get the other location.  



# Binary Search Tree
## Introduction 
One optimal of comparison based searching algos for a **sorted** array is binary search. Find the mid point of the current search subarray and eliminate half of all candidates at a time.  
Worst-case number of comparisons is $\log_2n+1$.  
Although search in hash table can be as fast as $O(1)$, but hash table can't be used for predecessor and successor searches.  
```python
def search(nums, target):
    low, high = 0, len(nums)-1
    while low <= high:
        mid = (low+high)//2
        if target < nums[mid]:
            high = mid - 1
        elif target > nums[mid]:
            low = mid + 1
        else:
            return mid
    # target not in nums
    return -1
    # as low and high cross each other
    return high <- predecessor
    return low <- successor
```
Maintaining a sorted array as we insert and delete is expensive. Therefore although search is fast on sorted array, we need a better data structure that also support fast updates. Spoiler they are trees.   
## Binary Tree
### Abstract Data Type
#### Data
A tree with designated root node, the one with out parent.  
Each tree node has a left child and a right child.  
Tree nodes with out children are leaf nodes.  
> For extended binary trees, we treat the children of leaf nodes as special **extended nodes** rather than null pointers.  

### Properties
> Important: Please be consistent with the lecture nodes. Depth and height all start from 0.  
#### Depth
The depth of **root node** is **0**.  
The depth or level of a node is its distance from the root node.  
The depth of a binary tree is the max level of all its nodes.  
#### Height
The height of **leaf node** is **0**.  
The height of one given node is one more than the maximum height of its non-null child.  
The height of a binary tree it the height of its root node.  
height = max(left_child_height, right_child_height) + 1, which is a recursive definition.
#### Traversal Orders
Recursive definition and recursive implementation.  
The description of order refers to the order of root with respect to its children.   
1. Preorder: root, left subtree, right subtree
2. Inorder: left subtree, root, right subtree
3. Postorder: left subtree, right subtree, root
#### Number of Nodes
Max number of nodes at level $l$ is $2^l$.  
Total number of node in the tree with height $h$ is the sum of all levels $2^{h+1}-1$.  
> Note that depth and height starts from 0  
> ```
>  level  height
>    0      3              5
>                        /   \
>    1      2          2       8
>                     / \     /
>    2      1        1   4   7
>                       /
>    3      0          3
> ``` 
## Binary Search Tree
### Abstract Data Type
#### Data
Inorder search visits the nodes in ascending order.  
Binary search implicitly represents searching in a BST.  
Each node of the binary search tree contains:  
1. Data value
2. Pointers to objects representing left child and right child
3. Extra information for balancing the tree
#### Operation
1. Search for a target: If target == node value, return result, search left subtree for smaller values and right subtree for larger values.  
2. Insert a value: search for a value until reaches null pointer, insert there
3. Delete a value: search for the value, assume find value in node `v`
    - If `v` is leaf node, delete it
    - If `v` has one null child, replace it with its non-null child
    - If `v` has two child, copy data value of its predecessor or successor then delete predecessor or successor  
> The predecessor of one node is the right most child on its left subtree.  
> The successor of one node is the left most child on its right subtree.  

### Analysis
#### Search Time
At search iteration, we search one node then move on to next level until we find the target or exaust the nodes.  
Worst-case time for a search is $h+1$ where h is the height of the tree.  
> Note that height starts from 0, the height of a leaf node is 0.  
> ```
>  level  height
>    0      3              5
>                        /   \
>    1      2          2       8
>                     / \     /
>    2      1        1   4   7
>                       /
>    3      0          3
> ``` 
In a binary tree with `n` total number of nodes is small than maximum possible number of nodes in a binary tree of height `h`  
$$n \leq 2^{h+1}-1$$
$$\rArr h+1 \geq \log_2n$$
Therefore the worst case time is $h+1$ which is at least $O(\log_2n)$.  
To ensure the efficiency of binary search tree, we need to minimize its height.  
#### Delete and Insert
As seaching in BST is involved in deletion and insertion. The time complexity is the same.  
#### Analysis of Insertion
Worst-case we insert `n` elements in ascending or descending order, the tree would become a single sided list of height `n`.  
If we insert `n` elements in random order, all out come of trees are not equally likely.  
The average insertion cost is $O(\log n)$ if insertions are random. [Proof: Lecture Node 8-23 to 8-24](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes8-handout.pdf)

### Strategies to Minimize Height
Rebuild and rotation are two strategies used to balance binary trees. The lecture stresses rotation based methods. There are three tree implementations that utilize rotation to maintain tree height.  
1. AVL Trees
2. Red Black Trees
3. Rank-Balanced Trees, which covers WAVL, AVL and Red Black

> You should know the analysis of AVL trees and red-black trees:  
> 1. asymptotic worst-case cost of insertion
> 2. asymptotic worst-case cost and number of rotations on insertion
> 3. asymptotic worst-case cost and number of rotations on deletion


#### Rotations
Right rotation at node `B`, left child `A` becomes its parent, `B` becomes `A`'s right child.  
Note that `x, y, z` are subtrees.  
```
    B             A
   / \           / \
  A   ▲    =>   ▲   B
 / \  z         x  / \
▲   ▲             ▲   ▲
x   y             y   z
```
Left rotation is the inverse of right rotation.  
```
  A                 B  
 / \               / \ 
▲   B      =>     A   ▲
x  / \           / \  z
  ▲   ▲         ▲   ▲ 
  y   z         x   y
```
> Note that they are called left/right rotation at node **B**.  
> This is consistant through out lecture notes.  
## AVL Trees
### Abstract Data Type
At each node of AVL trees, the height difference of the left subtree and the right subtree is either 0 or 1. By this rule a AVL's height $h = O(\log (n))$, where `n` is the total number of tree nodes.  
Aside from the data, AVL tree node stores additional height information for balancing the tree.  
### Operation & Analysis
1. Insertion: First insert node as in regular binary search tree, insert new node as leaf node, then update its ancestors' height information, rebalance unbalanced nodes if needed. Two rotations are needed at most to rebalance the AVL tree.    
2. Deletion: Delete following the same principle as binary search tree, then do as many as $O(\log n)$ rotations to rebalance the AVL tree.  

## Red Black Trees
### Abstract Data Type
Rules of Red-Black Trees
1. Red-black tree is a type of extended binary tree.  
2. All nodes are colored either red or black.  
3. All external nodes (children of leaf nodes if we don't use null pointer) are colored black.  
4. The children of red nodes must be colored black.  
5. The path form every external nodes to the tree root must contain same number of black nodes.  
6. The black height of a node `x` is the number of black nodes from `x` to a external node (not matter which one since they are the same distance), not counting `x` itself.  
### Operation & Analysis
1. Insertion: Standard insertion as BST, new node replaces a external node, color new node red and assign two external nodes to it. To fix rule 3, recolor at most $O(\log n)$ nodes. To fix rule 4, rotate at most $2$ times.  
2. Deletion: Follows the same principle as standard BST, requires at most $O(1)$ rotations.  

### Comparison
```
┌──────────┬───────────────────────┬────────────────────────────┐
│          │       AVL             │    Red-Black               │
├──────────┼──────────┬────────────┼──────────┬─────────────────┤
│          │ Rotation │ Extra      │ Rotation │ Extra           │
├──────────┼──────────┼────────────┼──────────┼─────────────────┤
│  Insert  │ 2        │ N/A        │ 2        │ O(logN) Recolor │
├──────────┼──────────┼────────────┼──────────┼─────────────────┤
│  Delete  │ O(logN)  │ N/A        │ O(1)     │ N/A             │
└──────────┴──────────┴────────────┴──────────┴─────────────────┘
```

## Rank-Balanced Trees
### Abstract Data Type
Rank-balanced tree is a abstract generalization of AVL trees. Each node has a rank, which can be height but not necessarily.  
Rank difference of a node is $\text{rank(parent(node))}-\text{rank(node)}$, rank of its parent's minus its rank.  
For the rebalancing operation:  
1. Follow up to the root and adjust ranks
2. Rotate if needed
### Special Cases
1. With a specific set of constraints, we can achieve AVL
2. With a specific set of constraints, we can achieve Red-Black
3. Weak AVL (WAVL) Trees

## Weak AVL (WAVL) Trees
> From list of topics:  
> 1. Understant the basic properties that characterize WAVL trees  
> 2. In insertion and deletion, which node are we interested in 
> 3. Given pseudocode and apply the basic WAVL rules to adjust rank

### Abstract Data Type
#### Data
Each node has a rank related to its subtree height.  
As defined in rank-balanced trees, the **rank difference** of **a node** = $\text{rank(parent(node))}-\text{rank(node)}$
> The confusing term "Rank Difference" is the property of a single tree node, its the difference of rank between its parent and itself.  

Type of node:  
1. (1,1)-node, if its two children both have rd of 1
2. (2,2)-node, if its two children both have rd of 2
3. (1,2)-node, if one of its child has rd 1 and the other one has rd 2 
> To make things more confusing the type of nodes is about the properties of the nodes' children.  

Properties of WAVL Trees:  
1. Rank difference of any nodes are either 1 or 2.  
2. Every external node has rank 0.  
3. An internal node with two external children must be a (1,1)-node.  
> Don't confuse rank and rank difference. 
> Rank be marked in the node and rank difference can be marked on the edge to the node's parent.  

Example for understanding the properties:  
From bottom up, all external nodes represented by rectangles have rank 0 by definition.  
The node 3 is a internal node with two external children, it has to be a (1,1)-node, which means its two children both have rank difference of 1. Therefore the rank of node 3 has to be 1.  
The rank of node 3 is 1 and the rank of its external sibling is 0. The rank difference can either be 1 or 2 according to the restriction. Therefore the rank of node 5 can only be 2 to satisfy the properties.  
```
     5   
   /   \
  3     □
 / \
□   □
```
#### Operation
##### Insertion
> According to the list of topics, memerizing step 1-3 is enough.  

To insert a value into the WAVL tree, assuming there are no duplicates.  
1. Search value, arrive at external node.  
2. Replace the external node with a new node `x` containing the value
3. Give `x` two external nodes, and rank of `x` would be 1 by definition
4. If tree was previously empty, `x` is the new root, end of insertion
5. Else `p` is the parent of `x`  
6. If `p` has rank 2, condition satified, end of insertion
    ```
         p
       /   \
      q     □
     / \    ▲
    □   □   x

    Successful insertion, same situation if the tree is mirrored.        
    ```
7. Else a violation will occur, as rank difference of `x` is 0 after insertion, fix needed.     
    ```
         p
       /   \
      □     □
            ▲
            x

    A rd-0 violation, fixing involves two cases depending on the rd of x's sibling s.
    If rd of s is 1, promote p
    IF rd of s is 2, rotation, two more subcases
    ```
##### Deletion
To delete a value from the WAVL tree  
1. Perform a standard BST deletion, the successor/predecessor fashion
2. Let `v` be the node that is actually deleted, the successor or predecessor, this is the actual node of interest
4. `p` is the parent of `v`
5. `s` is the sibling of `v`
6. `v` has at least one external node as its child, say `u`
7. let `x` be the other child of `v`, deletion may cause rd of `x` to become 3, rd-3 violation
8. If `x` is a internal node
    ``` 
            p
          /   \
         v     s     v is the node that actually got deleted
       /   \
      □     x
      ▲     
      u     

    The only possible rank arrangement is
        Rank(v) = 2
        Rank(u) = 0
        Rank(x) = 1
        If v is root p is null, otherwise Rank(p) = 3 or 4
    ```
9. If `x` is a external node 
    ``` 
            p
          /   \
         v     s    v is the node that actually got deleted
       /   \
      □     □
      ▲     ▲
      u     x

    The only possible rank arrangement is
        Rank(v) = 1
        Rank(u) = 0
        Rank(x) = 0
        If v is root p is null, otherwise Rank(p) = 2 or 3
    ```
TF is this nightmare, I'm not going any deeper.  

#### Analysis
The cost of fixing violations after insertion and deletion is $O(\log n)$ with $O(1)$ rotations.  

# Special Balanced Trees
## Recap & Conclusion of Binary Search Tree
Each node contains one key.  
Gurantee $O(\log n)$ **worst-case** time on search, insertion and deletion.  
Operations can be complicated if we need to maintain the BTS.  

Alternative approaches for tree structured search data structures can have following properties:  
1. Vaiable number of keys pre tree node
2. Randomization strategies for improve average case performance
3. Amortized alternatives, improve amortized performance

## Multiway Trees
In a multiway tree, each node have multiple keys and multiple children. 
$$\text{keys} = \text{children} - 1$$  
Binary tree is a special case where each node have 1 key and 2 children.  
```
            ┌───┬───┬───┬───┬───┐
            │k_1│k_2│k_3│...│k_r│
            ├───┼───┼───┼───┼───┤
            │   │   │   │   │   │
           T_0 T_1 T_2 ... ... T_r

Total number of r keys and r+1 children per node
In lecture note key k is 1-indexed & children T is 0-indexed
```
### B-tree or (a, b)-tree
#### Abstract Data Type
(a, b)-tree is a multiway-tree with restrictions on the maximum and minimum number of **children**.  
A (a, b)-tree is also called a B-tree of order `b`.  
> Important not to confuse: The parameter `a` and `b` refers to the number of **children**!  

`a` and `b` are positive integers, `b = 2a - 1`.  
The root has at least 2 children, in other word the root has at least 1 key.  
Each the node has a minimum of `a` children (except root) and a maximum of `b` children inclusive.  
$$\text{Number of children} \in [a, b]$$  
Which means each node has a minimum of `a-1` keys and a maximum of `b-1` keys inclusive.  
$$\text{Number of keys} \in [a-1, b-1]$$  
#### Operation
##### Search
Use predecessor search within the node. Find the potential child node that may contain the target. Until find node or reach null leaf node.  
##### Insertion
Insert key into a node (block as denoted by the lecture node) may cause it to overflow.  
A node should have `[a, b]` number of children and `[a-1, b-1]` keys.  
An overflow will cause a node/block to have `b` keys, `b = 2a - 1 = 2(a-1) + 1`.  
In this case split the overflown block into 2 blocks of `a-1` keys and push the 1 leftover key (the median) into the parent block as the new dividing key.  
> Insertion may cause a series of overflows and splits towards the root, handle recursively.  

##### Deletion
If key `k` to be deleted in not in leaf node, replace it with its immediate predecessor or successor then delete its predecessor or successor. This ensures that actual deletion always happen in leaf node.  
Delete key from a node may cause it to underflow.  
A node should have `[a, b]` number of children and `[a-1, b-1]` keys.  
An underflow will cause a node/block to have `a-2` keys.  
Try to get one key from one of its neighbours to refill to `a-1` keys. Two options where to look.   
1. The right most key from its left neighbour
2. The the left most key from its right neighbour
3. Push neighbour to parent as the new discriminating key to divide two siblings, move the original divider key to the underflowing block  
> The new borrowed key is not actually from its neighbour, its from the node's parent.  

If the only neighbour has a minimum number `a-1` keys, borrow one key will cause neighbour to also underflow.  
In the case above, we merge two neighbours with `a-1` keys and `a-2` keys, then we borrow the discriminating key from their parent. In total the new node will have `(a-1) + (a-2) + 1 = (2a-1) - 1 = b - 1` keys, which satifies the maximum amount of keys.  

> Insertion may cause a series of merge.  

#### Analysis
All operation access $O(\log_a n)$ nodes, which is one node per level. `n` is the total number of keys and `a` is the minimum number of keys per node.  
#### Application of B-tree
Useful for secondary storage.  

## Alternatives to Balanced Trees with Randomization
Use randomization strategies to lower the height of the search trees.  
1. Treaps
2. Skip lists
3. Zip trees
## Treaps
Tree + Heap
### Abstract Data Type
#### Data
A binary tree with fixed priority values between `[0,1]` generated and assigned to tree nodes when they are created.  
Organize tree nodes so that the structure forms:  
1. Binary Search Tree with respect to node key values
2. Heap-ordered Tree with respect to the priority values

> Note that contrary to the min-heap convention, treaps use high values to represent high priorities.  
> Higher priorities (larger values) are closer to the heap top.  

#### Operation
##### Insertion
Assuming we are try to insert value `k` into the treap.  
1. Create a tree node `v` with `k` as its key, and assign it a randomly generated priority value.  
2. Insert `v` to treap the same way as insertion in BST, insert as leaf node
3. Adjust the tree to make sure it complies with the heap order:  
    While v is not root and v.priority > its parent's priority
    - If v is left child, do a right rotation at its parent
    - If v is right child, do a left rotation at its parent

##### Deletion
Treap deletion diverges from BTS a little bit.   
To delete a target node `v`, we first move `v` down to leaf while preserving the heap-order, then delete `v` which is now a leaf node.  
While `v` is not a leaf node (which means `v` have at least one non-null child):  
1. If `v.left is None`: rotate left at `v`, let its right child replace its position
2. If `v.right is None`: rotate right at `v`
3. If `v` have both left and right child:  
   1. If `v.left.priority > v.right.priority`: right rotation
   2. If `v.left.priority < v.right.priority`: left rotation  
> No longer cares about `v`'s value durning the process since we are going to delete it anyway.  

Then delete the left node `v`.  

### Analysis
Expected time of search, insert, delete is $O(\log n)$.  
Expected number of rotation is $\leq 2$

## Skip Lists
### Abstract Data Type 
Series of lists, each list has a sublist of its successor.  
Decisions are randomly (just like how I do IRL) if a node on a list should appear in its preceding list.  
Last list in series contains the full set of keys.   

```
         ┌──┐              ┌──┐
────────►│5 ├─────────────►│17├───────►
         └─┬┘              └─┬┘
           │                 │
         ┌─┴┐        ┌──┐  ┌─┴┐
────────►│5 ├───────►│13├─►│17├───────►
         └─┬┘        └─┬┘  └─┬┘
           │           │     │
   ┌──┐  ┌─┴┐  ┌──┐  ┌─┴┐  ┌─┴┐  ┌──┐
──►│3 ├─►│5 ├─►│6 ├─►│13├─►│17├─►│20├─►
   └──┘  └──┘  └──┘  └──┘  └──┘  └──┘
```

### Analysis
Expected time of search, insert, delete is $O(\log n)$.  
Expected space $O(n)$.  


## Zip Tree
Zip tree was implemented in lab 3.  
### Abstract Data Type
#### Data
Zip tree is random search tree thats similar to treap. Rank of zip tree node is generated when the node is created.   
The rank of zip tree nodes follows geometric distribution and same rank values are allowed.  
Rank 0 with probability $\frac{1}{2}$  
Rank 1 with probability $\frac{1}{4}$  
Rank 2 with probability $\frac{1}{8}$  
Rank n with probability $\frac{1}{2^{n+1}}$  
Properties of node `v`:  
1. Rank of `v` is larget than rank of its left child (if exist), `v.rank > v.left.rank`
2. Rank of `v` is greater or equal to the rank of its right child, `v.rank >= v.right.rank`
3. Tie breaker: if `v` and `w` have the same rank, the one with larger key is on the **right subtree** of the node with smaller key.  

Zip tree capture same information as skip list, as the rank in zip tree can be interpreted as height in skip list.  

Instead of more complicated rotations, zip tree uses merging and unmerging of paths for its insertions and deletions, which are referred to as **zipping** and **unzipping**.  
#### Operation
##### Insertion
To insert a key `k` into the zip tree:  
1. Generate a node `x` with `k` as its key, assign a rank to the node
2. Proceed along the BST search path for `x.key` until we encounter the node `y` with following properties, `y` would be the node we'd like to replace.  
    - The first node in search path where its rank is smaller, `y.rank < x.rank`
    - **OR** the first node that have equal rank and larger key, `y.rank == x.rank and y.key > x.key`  
    > Just do regular search as if searching in BST if the nodes on the path have higher ranks. We only care about the nodes with smaller or same rank here.      
3. **Unzip** the reminder of the search path for `x.key` from node `y`, divide the subtree with `y` as root into two paths
    - `P` contains all the nodes with keys less than `x.key`
    - `Q` contains all the nodes with keys greater than `x.key`
4. Make top node of `P` the left child of `x` and top node of `Q` the right child of `x`
5. Insert `x` into the zip tree in `y`'s original spot

##### Unzipping
Using the example in Insertion. The unzipping process produces two paths `P` and `Q`.  
The key of nodes on path `P` is in increasing order, with non-increasing rank. All the left subtrees of nodes on `P` are preserved. The path contains `k`'s predecessor.      
The key of nodes on path `Q` is in decreasing order, with decreasing rank. All the right subtrees of nodes on `Q` are preserved. The path contains `k`'s Successor.  
```python
# x is the new node, y is the node to be replaced
def unzip(x: TreeNode, y: TreeNode):
	# returns top of P, top of Q
	def unzip_lookup(k, node):
		if node is None:
			return (None, None)
		if node.key < k:
			# proceed with right subtree
			(P, Q) = unzip_lookup(k, node.right)
			# connect nodes on path P
			node.right = P
			return (node, Q)
		#  node.key > k
		else:
			(P, Q) = unzip_loopup(k, node.left)
			node.left = Q
			return (P, node)
	return unzip_loopup(x.key, y)
```
As the node ranks are non-increasing or decreasing, we don't need to care about rank when connecting nodes back to corresponding path.  
##### Deletion
To delete a node `x` from zip tree:  
1. Search for `x` on  zip tree
2. Zip the following two paths
   - The path from `x.left` to the node containing predecessor of `x.key`
   - The path from `x.right` to the node containing successor of `x.key`
> Inorder predecessor: Following the right edges of the left subtree  
> Inorder successor: Following the left edges of the right subtree

3. Replace `x` with the top of zipped paths

##### Zipping
The inverse of the unzip operation. However we need to take ranks into consideration this time.  
Continuing from the Deletion example, denote `P` the path to predecessor and `Q` the path to successor. The only property we can use is that all keys of nodes from `P` is less than those from `Q`.  
```python
# x is the node to delete
def zip(x):
	# returns top
	def zipup(P, Q):
		if P is None:
			return Q
		if Q is None:
			return P
		if P.rank < Q.rank:
			Q.left = zipup(P, Q.left)
			return Q
		#  P.rank >= Q.rank
		else:
			# this complies with the tie breaker
			# same rank make the one with larger key on left subtree
			P.right = zipup(P.right, Q)
			return P
	return zipup(x.left, x.right)
```
### Analysis
No operation complexity provided in the lecture notes.  
#### Zip Tree and Skip List
Zip trees capture the same information as skip lists.  
Zip tree rank is skip list height.  
Zip trees don't need to move back when searching unlike skip lists.  
#### Zip Tree and Treap
Both are binary search trees on keys and max-heap on priorities.  
Treaps use rotation to rebalance, zip trees use zip and unzip.  
Priorities:  
1. Treap
   - Uniform distribution
   - Unique priority values
   - $O(\log n)$ bits for representing priority value
2. Zip Tree
   - Geometric distribution
   - Allow tie in rank
   - $O(\log \log n)$ bits for representing priority value

## Alternatives that Improve Amortized Performance
## Splay Trees
### Abstract Data Type
Splay trees are binary search trees with no explicit balance condition and stretegy. We hope the tree to stay at resonable height through out our searches, insertions, deletions with the help of splay operation.  
Splay at node `x` means making `x` the root though a **specific** series of rotations.  

### Operation
Three cases that required different and specific rotations
#### Case 1 zig
A node `x` with no grandparent 
```
Case 1 L zig
    y            x
   / \          / \
  x   ▲   =>   ▲   y
 / \              / \
▲   ▲            ▲   ▲
```
1. Rotate right at x
```
Case 2 R zig
  y                x
 / \              / \
▲   x     =>     y   ▲
   / \          / \
  ▲   ▲        ▲   ▲
```
1. Rotate left at x
#### Case 2 zig-zig
A node `x` is LL or RR grandchild
```
Case 1 LL zig-zig
      z            x
     / \          / \
    y   ▲        ▲   y
   / \              / \
  x   ▲     =>     ▲   z
 / \                  / \
▲   ▲                ▲   ▲
```
1. Rotate right at z
2. Rotate right at y
```
Case 2 RR zig-zig
  z                    x
 / \                  / \
▲   y                y   ▲
   / \              / \
  ▲   x     =>     z   ▲
     / \          / \
    ▲   ▲        ▲   ▲
```
1. Rotate left at z
2. Rotate left at y
#### Case 3 zig-zag
A node `x` is LR or RL grandchild 
```
Case 1 LR zig-zag
    z
   / \               x
  y   ▲            /   \
 / \              y     z
▲   x      =>    / \   / \
   / \          ▲  ▲  ▲   ▲
  ▲   ▲
```
1. Rotate left at y
2. Rotate right at z
```
Case 2 RL zig-zag
   z
  / \                x
 ▲   y             /   \
    / \           z     y
   x   ▲   =>    / \   / \
  / \           ▲  ▲  ▲   ▲
 ▲   ▲
```
1. Rotate right at y
2. Rotate left at z

> [Review the names for rotations](#rotations)  

#### Search
Search for key `k` in splay tree with the regular BST seach method.  
Splay node if found node with key `k`.  
If not found, splay the last node on search path.  
#### Insert
Insert with regular BST insertion, insert node as leaf, then splay the newly inserted node.
#### Delete
Search for key in splay tree, then replace it with inorder predecessor or successor. Delete its predecessor or successor.  
Splay the parent of the leaf node we deleted.  
If not found, splay the last node on search path.  

### Analysis
All three operations can be performed with $O(\log n)$ **amortized time**.  
Not that its not worst-case time or average-case time.  
[Proof of time complexity.](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes10-handout.pdf)

# String Matching
String is a finite sequence of symbols from a alphabet. String matching problem is to find the occurance and position of substring in a large document.  
One of the subproblem of string matching is the dictionary problem.  
## Dictionary Problem
Dictionary and successor problem on strings. Given an word `W` and a list of words `L`.  
We'd like to check if is `W` in `L`, whats the next word after `W`, is `W` part of some words in `L`.  
Data structure for dictionary problem: 

## Prefix Tree
### Abstract Data Type
A tree with its leaf nodes containing words from `L`.  
Parent `p` node of a node `x` contains a string from `x` with its last symbol removed.  
Tree contains all possible prefixes of words in `L`.  

### Operation
User $\epsilon$ to represent the empty prefix string as the tree root.  
Search, find successor, insert a string all very easy.  
### Analysis
Prefix tree contains a lot of redundant information, prefix can be implicitly stored in the search path.  

## Trie
### Abstract Data Type
Instead of the entire prefix string, each node stores a dictionary mapping symbols to child nodes.  
To avoid ambiguity, we mark the end of word with $.  
### Operation
1. Test membership: search if string in trie, from root search each symbol in string down the trie
2. Insert: for each symbol in string, follow the path in trie if symbol already exist, else create new path
### Analysis 
$O(|s|)$ lookup time where $|s|$ is the length of the string.  
The time complexity of creating a trie from `L` and the space for trie are both $O(\text{sum of word length in L})$  

## Compressed Trie
From trie we can further improve the space usage by eliminate nodes with only one child.  
The result is a **compressed trie**.  
### Abstract Data Type
Each node we map alphabet **symbols** to child nodes.  
Incoming string has to match all the symbols before continuing downward.  

## Suffix Tree
A suffix tree for string `s = s[0]s[1]...s[n-1]$` is a trie with end marker for all suffixes of `s`.  
All suffixes of `s`:  
```
s[0]s[1]...s[n-1]$
	s[1]...s[n-1]$
		...
		   s[n-1]$
		   		 $
```
Suffix tree is a prefix tree for all suffixes, which means it contains all `s`'s substrings.   
Mark the leaf node with the start point of that specific suffix string.
### Operation
Test if target string `q` is a substring of `s`.  
Search down the suffix tree without mismatch, continue down the descendent tree until leaf **nodes** to find the starting point of the substring.  
> The descendent tree from endpoint of `q` can have multiple leaf nodes, which meaning substring `q` has multiple occurance in `s`.  


## Compressed Suffix Tree
Eliminate nodes with only one child.  
Same search method as uncompressed suffix tree.  

## Compacted Suffix Tree
The number of nodes in a string's compacted suffix tree $\leq 2\times \text{length of string}$  
In this case its actually compacted compressed suffix tree.  
### Abstract Data Type 
Each node stores:  
Map alphabet symbols to child nodes, space required $O(\text{number of children})$.  
> Number of children means number of possible subtrings from current node.  

Instead of storing compressed substrings, store `[start pos, length]` instead to save more space.  

## Compressed vs Compacted
1. Compressed means a chain of nodes with one child are compressed into a single node
2. Compacted means text of incoming string in a node is replaced by its start position and length 

# Range Queries
Given some data points in some space, report aggregate information about points within some query range.  
```
 │
 │         .
 │            .          .
 │    .
 │                  .
 │      .        .
 │          .
 │  .      .            .
─┼────────────────────────────
 │
```
Example type of range queries:  
1. Number of data points
2. List all points
3. Minimum, maximum 
4. Average, standard deviation

## Decomposable Range Queries
A range query is decomposable if there is some binary operation $\otimes$ that  
$\otimes$ is associative and has an identity operand  
Query range `Q` can be split two disjoint regions `X` and Y  
$$\text{Query}(Q) = \text{Query}(X)\otimes \text{Query}(Y)$$

Examples of decomposable range queries:  
1. Counting: $\otimes$ is $+$
2. Sum: $\otimes$ is $+$
3. Listing: $\otimes$ is $\cup$
4. Min, Max: $\otimes$ is $\min, \max$

Example of query that's not decomposable:  
Average, knowing the average of two disjoint regions is not enough. An work around is to query the sum and the count, which are decomposable, then compute the average.  

> Count is the number of data points, sum is the sum of all data values.  

## Dynamic 1-Dimentional Decomposable Range Queries
Dynamic means:  
Data points can be inserted or deleted.  
1-Dimentional means:  
Data with real numbers as keys, the search range is closed interval `[L, R]`.  
