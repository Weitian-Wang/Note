- [Introduction](#introduction)
  - [Focus of the course](#focus-of-the-course)
  - [Abstract Data Type vs Data Structure](#abstract-data-type-vs-data-structure)
    - [Abstract Data Type](#abstract-data-type)
    - [Data Structure](#data-structure)
    - [Examples](#examples)
- [Analysis of Data Structures](#analysis-of-data-structures)
  - [Worst-case](#worst-case)
  - [Average-case](#average-case)
  - [Amortized Analysis](#amortized-analysis)
    - [Amortized Analysis with Potential Function Method](#amortized-analysis-with-potential-function-method)
- [Array](#array)
  - [Abstract Data Type](#abstract-data-type-1)
    - [Data](#data)
    - [Operations](#operations)
- [Array List - Dynamic Arrays](#array-list---dynamic-arrays)
  - [Abstract Data Type](#abstract-data-type-2)
    - [Data](#data-1)
    - [Operations](#operations-1)
  - [Implementation](#implementation)
    - [Data](#data-2)
    - [Operation \& Amortized Analysis](#operation--amortized-analysis)
      - [Increment](#increment)
      - [Decrement](#decrement)
- [Stack](#stack)
  - [Abstract Data Type](#abstract-data-type-3)
    - [Data](#data-3)
    - [Operations](#operations-2)
  - [Implementation](#implementation-1)
- [FIFO Queue](#fifo-queue)
  - [Abstract Data Type](#abstract-data-type-4)
    - [Data](#data-4)
    - [Operations](#operations-3)
- [Deque - Double-Ended Queue](#deque---double-ended-queue)
  - [Abstract Data Type](#abstract-data-type-5)
    - [Data](#data-5)
    - [Operations](#operations-4)
- [Dictionary Problem](#dictionary-problem)
  - [Abstract Data Type](#abstract-data-type-6)
    - [Data](#data-6)
    - [Operations](#operations-5)
- [Hashing](#hashing)
  - [Basics](#basics)
    - [Load Factor](#load-factor)
    - [Hash Function](#hash-function)
      - [k-independent Hashing](#k-independent-hashing)
        - [Generate k-independent Hash Functions](#generate-k-independent-hash-functions)
        - [Usage](#usage)
      - [Tabulation Hashing](#tabulation-hashing)
        - [Generate Tabulation Hash](#generate-tabulation-hash)
        - [Implementation](#implementation-2)
    - [Rehashing](#rehashing)
  - [Hash Collision](#hash-collision)
    - [Hash Chaining](#hash-chaining)
      - [Operations](#operations-6)
      - [Advantage](#advantage)
      - [Disadvantage](#disadvantage)
    - [Linear Probing](#linear-probing)
      - [Analysis](#analysis)
      - [Code](#code)
      - [Disadvantage](#disadvantage-1)
    - [Quadratic Probing](#quadratic-probing)
    - [Double Hashing](#double-hashing)
    - [Cuckoo Hashing](#cuckoo-hashing)
      - [Visualizing Cuckoo Hashing](#visualizing-cuckoo-hashing)
        - [Directed Graph](#directed-graph)
        - [Undriected Graph](#undriected-graph)
      - [Loop Detection](#loop-detection)
      - [Analysis](#analysis-1)
- [Priority Queue](#priority-queue)
  - [Abstract Data Type](#abstract-data-type-7)
    - [Data](#data-7)
    - [Operation](#operation)
  - [Data Structure](#data-structure-1)
    - [Naive List](#naive-list)
    - [Binary Heap](#binary-heap)
      - [Operations](#operations-7)
      - [Analysis](#analysis-2)
    - [K-ary Heap](#k-ary-heap)
      - [Operations](#operations-8)
    - [Fibonacci Heap](#fibonacci-heap)
      - [Operations](#operations-9)
      - [Analysis](#analysis-3)
        - [Amortized Time of Decrease Priority](#amortized-time-of-decrease-priority)
        - [Amortized Time of Delete Min](#amortized-time-of-delete-min)
  - [Application](#application)
    - [Dijkstra's Algorithm](#dijkstras-algorithm)
      - [Dijkstra's Algorithm with Binary Heap](#dijkstras-algorithm-with-binary-heap)
      - [Dijkstra's Algorithm with K-ary Heap](#dijkstras-algorithm-with-k-ary-heap)
      - [Dijkstra's Algorithm with Fibonacci Heap](#dijkstras-algorithm-with-fibonacci-heap)
    - [Heap Sort](#heap-sort)
- [Set](#set)
  - [Abstract Data Structure](#abstract-data-structure)
    - [Data](#data-8)
    - [Operation](#operation-1)
  - [Implementation](#implementation-3)
    - [Bitmap](#bitmap)
      - [Bitmap Set Operations](#bitmap-set-operations)
      - [Analysis](#analysis-4)
    - [Set in Python - Hash Implementation](#set-in-python---hash-implementation)
      - [Operations](#operations-10)
- [Bloom Filter](#bloom-filter)
  - [Abstract Data Type](#abstract-data-type-8)
    - [Data](#data-9)
    - [Operation](#operation-2)
  - [Application](#application-1)
  - [Analysis](#analysis-5)
    - [Advantages](#advantages)
    - [Disadvantages](#disadvantages)
    - [False Positive Rate](#false-positive-rate)
- [Cuckoo Filter](#cuckoo-filter)
  - [Abstract Data Type](#abstract-data-type-9)
    - [Data](#data-10)
    - [Operation](#operation-3)
    - [Cuckoo Filter vs Cuckoo Hashing](#cuckoo-filter-vs-cuckoo-hashing)
  - [Implementation Details](#implementation-details)
    - [Calculate Positions](#calculate-positions)
      - [Calculate Position for Search](#calculate-position-for-search)
      - [Calculate Second Position From The First Position](#calculate-second-position-from-the-first-position)
      - [Calculate First Position From The Second Position](#calculate-first-position-from-the-second-position)
- [Binary Search Tree](#binary-search-tree)
- [Special Balanced Trees](#special-balanced-trees)
  - [B-trees/Multiway Trees](#b-treesmultiway-trees)
- [Alternatives to Balanced Trees with Randomization](#alternatives-to-balanced-trees-with-randomization)
  - [Treaps](#treaps)
  - [skip lists](#skip-lists)
  - [zip trees](#zip-trees)


# Introduction
## Focus of the course
Anlyze the performance of the algorithm implemented with given data structure.  
## Abstract Data Type vs Data Structure
### Abstract Data Type
Abstract data type defines the **logical** form of the data type. We care about the data type and operations about ADT.
### Data Structure
The data structure implements the **physical** form of the data type.  
### Examples
Dictionary
Abstract Data Type:  
- Data type: key-value pairs  
- Operations: 
  - Query: find value associated with a given key
  - Update: store value of a given key  

Data Structure:  
The actual implementation of the ADT above:  
- Hashing schemes
- Balanced binary search trees

# Analysis of Data Structures
## Worst-case
Restrictive time used for real-time response time analysis.
## Average-case
Expected time value, taking input probilities in to account. As we are making assumption about the probilities, average-case time can be inaccurate.
## Amortized Analysis
**Worst** case time for a **sequence** of operations.  
$$\text{Total Actual Time} \leq \text{Total Amortized Time}$$
Worst-case amortized time is the upper bound of worst-case actual time. [Proof see note 2-15](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes2-handout.pdf).  
### Amortized Analysis with Potential Function Method
1. Define a non-negative potential function $\Phi$, describing the states of the data structure. Initially $\Phi=0$
> Think potential function as the distance between current state of the data structure and the idea state. One analogy would be the gravitational potential energe.  

2. Define amortized time of an operation:
$$\text{amortized time} = \text{actual time} + C\times(\Phi_{new}-\Phi_{old})$$
> The amortized time equals actual time plus change of potential. If change of potential is positive, the data structure is further from its idea state, which is going to cost more time for future operations.  

> The potential is 0 when the data structure is initialized.  
> The potential of the data structure is non-negative(zero or positive).    
> The change of potential $\Delta\Phi=\Phi_{new}-\Phi{old}$ may be negative.
> The actual time of an operation is always positive.
> The amortized time of an operation may be negative, which will make it O(1).

# Array
## Abstract Data Type
### Data
Contiguous store of data items, with fixed length
### Operations
1. Create array of length n, 0-indexed by convention
2. Store a given value at a given index
3. Retrieve value stored at a given idex


# Array List - Dynamic Arrays
## Abstract Data Type
### Data
Continuous store of data items, with variable length. Combines the functionalities of arrays and linked lists.  
### Operations
1. Create a new ArrayList of length n
2. Return length of the current ArrayList
3. Store an item in given location index i
4. Reterieve the item stored in location index i
5. Increase the current length by 1
6. Decrease the current length by 1
## Implementation
Use fixed length array to implement ArrayList
### Data
Current number of elements (or length) L.  
Underlying array B, with $|B|\geq L$.
> $|B|$ denotes the length of array B.  

Keep $|B|/4 \leq L \leq |B|$, if smaller than $|B|/4$ shrink size to idea size, if larger than $|B|$ expand size to idea size.     
Ideal state $L = |B|/2$
### Operation & Amortized Analysis
Given that the ideal state of this data structure implementation is $L = |B|/2$, we can define the potential function
$$\Phi=|2L-|B||$$
#### Increment
Increase the length of ArrayList by 1, if L is too big that it exceeds $|B|$, reallocate underlying array.
```
L = L + 1
if L > sizeof(B):
  B_NEW = allocate new array of size 2*L
  copy B in to B_NEW
  set remaining location of B_NEW to null
  B = B_NEW
```
Amortized time for increment operation where B is resized:  
$\text{Potential Fucntion} \Phi=|2L-|B||$  
$\text{Amortized Time} = \text{Actual Time} + C\times \Delta\Phi$  
$\text{Actual Time} \leq c\times(L+1)$ for copying $L+1$ elements.  
$\Phi_{old}=|2L-|L||=L$  
$\Phi_{new}=|2L-|2L||=0$  
$\text{Amortized Time} \leq c\times(L+1) + c\times(\Phi_{new}-\Phi_{old})$  
$=c\times(L+1)+c\times(-L)=c=O(1)$  
#### Decrement
Decrease the length of ArrayList by 1, if L is smaller than $|B|/4$, resize the underlying array to $2L$.  
Amortized time for decrement operation where B is resized:  
$\text{Potential Fucntion} \Phi=|2L-|B||$  
$\text{Amortized Time} = \text{Actual Time} + C\times \Delta\Phi$  
$\text{Actual Time} \leq c\times(L-1)$ for copying $L-1$ elements.  
$\Phi_{old}=|2L-|4L||=2L$  
$\Phi_{new}=|2L-|2L||=0$  
$\text{Amortized Time} \leq c\times(L+1) + c\times(\Phi_{new}-\Phi_{old})$  
$=c\times(L+1)+c\times(0-2L)=-cL=O(1)$  
> As per lecture note, negative amortized time is valid, and quantity would be $O(1)$
```
L = L - 1
if 4*L < sizeof(B):
  B_NEW = allocate new array of size 2*L
  copy data, set other locations
  B = B_NEW
```

# Stack
## Abstract Data Type
### Data
Contiguous store of data items, with variable length and FILO/LIFO properties.
### Operations
1. Create an empty stack
2. Push: insert an item at the top of the stack
3. Pop: remove the item at the top of the stack
## Implementation
Stack can be implemented with ArrayList. When using implementing something with known data structure, we can use its amortized operation times as new base operation times.

# FIFO Queue
## Abstract Data Type
### Data
Contiguous store of data items, with variable length and FIFO properties.
### Operations
1. Create an empty queue
2. Enqueue: insert an item at the rear of the queue
3. Dequeue: Remove the item at the front of the queue

# Deque - Double-Ended Queue
## Abstract Data Type
### Data
Contiguous store of data items, with combined functionality of stack and queue.
### Operations
1. Create and empty deque
2. Insert an item at the front of the deque
3. Insert an item at the rear of the deque
4. Remove an item at the front of the deque
5. Remove an item at the rear of the deque

# Dictionary Problem
## Abstract Data Type
### Data
Collection of key-value pairs.  
Keys can be numbers, string, memory address, etc.  
Values can be basic data types or references.
### Operations
1. Search value associated with given key
2. Update value of given key, or add new pair if key not previously exist in the dictionary
3. Delete pair associated with given key

# Hashing
Suppose we have n key-value pairs, n may change as we perform set/delete operations.  
Maintain a hash table H of size N > n, N may require resizing as n changes.
## Basics
### Load Factor
Load factor is the number of elements in a hash table divided by the total number of table slots.  
$$\alpha=\frac{n}{N}$$
### Hash Function
For $K$ possible keys to $N$ possible index values.  
Hash function h: $\text{keys}\rarr\text{indices in hash table}$  
#### k-independent Hashing
Definition of k-independent: for every $k$-tuple of keys, all $N^k$ $k$-tuples of indice values are equally likely.
##### Generate k-independent Hash Functions
1. Choose a prime $p\gg N$
2. Randomly choose k numbers $a_0, a_1, \dots, a_{k-1}$ in range $[0, p-1]$
3. User polynomial to generate hash value $h(x)=((a_0+a_1\cdot x+a_2\cdot x^2+\dots+a_{k-1}\cdot x^{k-1})mod\ p)mod\ N$
##### Usage
- Hash chaining: 2-independence for O(1) expected time
- Linear probing: 5-independence
- Cuckoo hashing: $O(\log n)$-independence
#### Tabulation Hashing
##### Generate Tabulation Hash
Assume keys are 32-bit integers -> 4 byte -> 4 8-bit segmentations
1. Preprocessing: Build four tables $T_i[\cdot]$ of 256 random numbers each.
2. h(k):
    - Partition k in to 4 bytes: $k_0, k_1, k_2, k_3$
    - Return $T_0(k_0)\oplus T_1(k_1)\oplus T_2(k_2)\oplus T_3(k_3)$
> $\oplus$ bitwise XOR
##### Implementation
Above tabulation hash function is 3-independent.
### Rehashing
If we need to resize H to accomodate more elements, we need choose a new hash function that map all possible key to new N indices.
## Hash Collision
Different hash key mapped to the same hash index value.  
Few strategies discussed in the lecture for dealing with hash collsions:
### Hash Chaining
Each cell of hash table H store a collection (as ArrayList or linked list) of key-value pairs.
#### Operations
Estimated time per operation = $O(1+\alpha)$.
> 1 comes from the inevitable hash opertion and collection lookup  
> The number of n-1 existing keys colliding with k is $(n-1)\times\frac{1}{N}$, assuming hash function generate N indices with the same likelihood.  
1. Search(key): first find the collection in the hash table, then scan through the collection, looking for the pair corresponds to given key.
2. Set(key, value): Update pair or add new pair to the collection.
3. Delete(key): delete the pair in the collection.
#### Advantage
Hash chaining is a simple solution that works.  
#### Disadvantage
Extra space for storage and slower access time.
### Linear Probing
Each cell of hash table H store one key-value pair.  
Try to store (k,v) in index position h(k), if full try h(k)+1, h(k)+2, so forth and wrap around modulo N.
#### Analysis
Omit the proof, the expect search time of hash table with linear probing is $O(1)$.  
Expected time for successful search
$$O\left(1+\frac{1}{(1-\alpha)}\right)$$
Expected time for unsuccessful search
$$O\left(1+\frac{1}{(1-\alpha)^2}\right)$$
#### Code
```
def search(k):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  // key == k
  if H[i] is non-empty:
    return H[i].value
  else:
    exception
```
```
def set(k,v):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  H[i] = v
```
A simple solution for deletion would be mark the deleted position with a flag, indicating the value is nolonger available.  
The following code is for moving probed elements to the front.
```
def delete(k):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  if H[i] is empty:
    exception
  j = (i+1)%N
  while H[j] is non-empty:
    if h(H[j].key) not in circular range [i+1, j]:
      move H[j] to H[i]
      i = j
    j = j + 1
```
i points to available empty spot in the left side  
j scan through current chunck of elements      
#### Disadvantage
Performance degrades as load factor $\alpha=\frac{n}{N}$ gets close to 1.
### Quadratic Probing
Instead of looking for next one slot, look for $h(k)+1$, $h(k)+4$, $h(k)+9$, etc.
### Double Hashing
User a secondary hash function $h_2(k)$, try $h(k)+h_2(k)$, $h(k)+2\times h_2(k)$, $h(k)+3\times h_2(k)$, etc. 
### Cuckoo Hashing
Two hash tables: $H_0$, $H_1$  
Two hash functions: $h_0$, $h_1$  
Search(k): Look in both places $H_0[h_0(k)]$, $H_1[h_1(k)]$  
Set(k,v): Start from table zero, insert at $H_0[h_0(k)]$, if pair $(k', v')$ is already there, evict it to $H_1[h_1(k')]$ and so forth.  
#### Visualizing Cuckoo Hashing
We visualize cells as vertices, keys as edges. 
##### Directed Graph
Direct each edge **towards** the cell that cintains the key.  
A graph is a valid state only if each vertex (cell) has at most 1 incoming edge.
##### Undriected Graph
If two vertices have three **paths**, there is no way to orient the edges so that each vertex has only one incoming edge.
#### Loop Detection
1. Loop time threshold: project 1 implementation
2. Explicitly check for cycle: homework 1 track the starting key, if we try to place it three times ($t_0$, $t_1$, $t_0$ again) we are in the loop.
#### Analysis
Guranteed $O(1)$ worst-case search time, at the cost of slower set operation.  
For a sequence of n operations, the expect number of rebuild is 1, rebuild takes $O(n)$, so expect time **per operation** is O(1).

# Priority Queue
## Abstract Data Type
API
### Data
A set of items with associated priorities. By convention the smaller numbers have the higher priorities.
### Operation
1. Create priority queue for a set of items
2. Add and remove items
3. Find the item with minimum value
4. Change the priority of an item
## Data Structure
Physical implementations of a priority queue can be lists of heaps.
### Naive List
Use unsorted lists or sorted lists to implement priority queues.  
Homework 2 represent the queue as a dynamic array of data values.  
### Binary Heap
An **array** $A$ of $n$ values interpreted as a complete binary tree.  
1. Root = $A[0]$  
2. Children($A[i]$)=$(A[2i+1],\ A[2i+2])$  
3. Parent($A[i]$)=$A[floor\left ( \frac{i-1}{2}\right )]$  
4. Height = $\log_2 (n)$
#### Operations
1. Find min: return $A[0]$
2. Sift down at index i: if $A[i]$ is larger than its descendants, move it down the tree. Worst-case $O(\log n)$.  
    ```
    def siftDown(i):
        // two comparison per iteration
        while A[i] larger than any of two children:
            swap A[i] with best child
            // update i continue sift down until smaller than both children
            i = index of best child
    ```
3. Sift up at index i: if $A[i]$ is smaller than its parent, move it up the tree. Worst-case $O(\log n)$.  
    ```
    def siftUp(i):
        // one comparison per iteration 
        while A[i] smaller than parent:
            swap A[i] with parent
            // update i continue sift up until ;arger than parent
            i = index of best parent
    ```
4. Delete item at given index $i$: move last element $A[n-1]$ to $A[i]$, then do both sift_up(i) and sift_down(i)
5. Delete min would be delete item at index 0
6. Make heap from an array A of n items
    ```
    def makeHeap():
        for i in n-1, n-2, ..., 1, 0:
            siftDown(i)
    ```
    > Makeheap analysis see [note 5-11](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes5-handout.pdf).  
    > Why we run siftDown on each position for creation rather than siftUp? Operation siftDown is costy for leaves heigher up in the tree, siftUp is costy for leaves closer to the bottom. There are more nodes closer to the bottom in a complete binary tree. Therefore siftDown is the faster choice.


#### Analysis
Make heap from a set of n items $O(n)$ time, all other operations have $O(\log n)$ time.
### K-ary Heap
Instead of two children, each node in a k-ary heap have k children.  
Suppose we have k-ary tree with n nodes.  
Height of tree: $\log_k n = \frac{\log k}{\log n}$  
Children($A[i]$) = $(A[ki+1],\ A[ki+2],\ A[ki+3], \dots, A[ki+k])$  
Parent($A[i]$) = $A[floor\left ( \frac{i-1}{k}\right )]$  
#### Operations
1. Sift up is the same as binary heap, do 1 comparison per level
2. Sift down: we need to find the minimum amongst all k children. Which makes it $O(k\cdot\text{height})$
### Fibonacci Heap
**Forest** of heap-ordered trees with pointer to the best tree root.  
Each node has:  
1. Data item and priority value
2. Pointer to parent node
3. Number of children
4. Pointer to a child node
5. Doubly-link pointers to left and right neighbors.
6. A Boolean flag, True when
    - node is not root
    - has one child removed  

Potential function $\Phi = (\text{number of root nodes} + 2 \cdot(\text{number of True flags}))$  

#### Operations
1. Find min: return item pointed be the min pointer, which points to the minimum value amongst all root nodes. $\Delta\Phi=0$.
2. Create new Fibonacci Heap from n items: create a root node for each item, link items. $\Delta\Phi=n$ as we created n more root nodes.
3. Insert item: create root node for the new item, update min pointer. $\Delta\Phi=1$ as we created 1 more root node.
4. Merge **two** Fibonacci Heaps: Concatenate root lists and update best root pointer. $\Delta\Phi=0$.  
> We are doing operation in the laziest fashion so far. The strategy is to delay the work until we can no longer ignore them.  

5. Decrease priority of target node x:
    - If x is root, decrease priority inplace
    - If x is not a root, make it x a root then decrease
        - If x's parent p is not root, and flag is False, set to True
        - If x's parent p is not root, and flag is True, make p a root, and so forth for p's parent.
    > Remove subtree from one tree may cause a chain reaction of node updates or promotions (become root node).  
  
    > Meaning of the boolean flag is to track when a non-root node loses its second child, it has to become a root.

6. Delete minimum: delete the root where minimum value resides, children of deleted root become roots and set the flags as False. Then improve the forest.
7. Improve: force all **root nodes** to have different numbers of children (at most $M=\log n$).
      ```
      // use array C to track roots' numbers of children
      // initially C is empty and R contains all roots
      // allocate array C of size M+1, let R be the collection not yet in C
      while R is nonempty:
          remove x from R
          // no children number conflict
          if C[x.childCount] == null:
              C[x.childCount] = x
          else:
              // set x as children of existing root
              combine x, C[x.childCount] to a new tree y
              // add new tree to working collection
              add y to R
              // remove existing root with same number of children as x
              C[x.childCount] == null
      ```
#### Analysis
##### Amortized Time of Decrease Priority
Let k be the number of promotions triggered by one decrease priority operation.  
Actual time: $O(k+1)$ for k promotions and decrease priority.  
With k promotions we created k new roots.   
With k promotions we also flipped k-1 (or k if taget node has True flag) flags from True to False.  
Change of potential $\leq k-2(k-1)+2=-k+O(1)$. Worst case amortized time = $O(1)$.
##### Amortized Time of Delete Min
Overall amortized time $O(M)$, where $M$ is the maximum number of children that a node is allowed to have. We choose $M=\log n$ to get $O(\log n)$ as amortized time for DeleteMin.
## Application
### Dijkstra's Algorithm
The given graph contains n vertices and m edges.  
Initialize a priority queue Q of vertices.  
```
// iterate n times
while Q is nonempty:
   find and remove minimum vertex v
   // iterate m times in total, not in one loop
   for each edge from v -> w:
      // update priority
      D[w] = min(D[w], D[v] + length(v->w))
```
#### Dijkstra's Algorithm with Binary Heap
Delete minimum is executed n times $O(n\log n)$.  
Updata priority is executed m time $O(m\log n)$.  
> Use additional dictionary to track the nodes so we don't need to search for the nodes in the array before changing priorities.  
  
> As for Dijkstra's algo we are only decreasing the priorities of nodes, therefore **sift up** would be sufficient to fix heap after change priorities or delete min.  

Total runtime of Dijkstra's algorithm with binary heap $O(n\log n + m\log n)$. For a connected graph m >= n - 1, which overshadows n. Hense time $O(m\log n)$. EXPLANATION NOT VARIFIED  

#### Dijkstra's Algorithm with K-ary Heap
Recap the level of k-ary tree with n nodes = $\log_k n = \frac{\log k}{\log n}$  

Delete min operation now cost $O(k\cdot\frac{\log n}{\log k})=O(\frac{k}{\log k}\cdot\log n)$, which is **slower** than binary heap's $O(\log n)$.  

Decrease priority operation cost one conparison each level, but with k-ary tree there are fewer levels. $O(1\cdot \text{depth of tree}) = O(\frac{1}{\log k}\cdot\log n)$, which is **faster** than than binary heap's $O(\log n)$.  

The total time $O(n\cdot\text{deleteMin}+m\cdot\text{decreasePriority})$ = $O(n\cdot\frac{k}{\log k}\cdot \log n + m\cdot\frac{1}{\log k}\cdot \log n)$.  

Proof ommited. Choose $k=\frac{m}{n}$ and when $m=n^c,\ c > 1$, the complexity simplifies to $O(m)$.

> Interpretation: for graph with number of edges that is exponential to the number of vertices, Dijkstra's algo with k-ary heap runs at $O(m)$ time.

#### Dijkstra's Algorithm with Fibonacci Heap
Recap decrease priority $O(1)$ and delete min $O(\log n)$.  
Total time for Dijkstra's algorithm with Fibonaccia Heap = $O(m+n\log n)$
### Heap Sort
Implement priority queue as a heap for a selection sort algorithm. 

# Set
## Abstract Data Structure
### Data
Each item is included only once without repetitions. For Python, only hashable items are allowed to be stored in a set.  
### Operation
1. Construct a set from a list of items
2. Union of two sets
3. Intersection of two sets
4. Compare two sets
5. Check if item in set
6. Add item to set
7. Remove item from set

## Implementation
### Bitmap
When the elements are non-negative integers `[0, 1, 2, ..., n-1]` total of `n` elements or can be numbered using small integers. We can use n-bits to represent the n-elements.  
The n-bits binary vector can be interpreted as a integer number.  
For example the set of `{0, 3, 6}` where elements `x = 6`, `y = 3` and `z = 0`, can be represented as following binary vector.  
```
bit  7 6 5 4 3 2 1 0

val  0 1 0 0 1 0 0 1
       ▲     ▲     ▲
ele    x     y     z
``` 
Integer value of the binary vector = $2^x + 2^y + 2^z$.
#### Bitmap Set Operations
1. Create `{x, y, z}`, `(1<<x)|(1<<y)|(1<<z)`, left shift 1 bit multiple times according to the element value, then bit-or the results.
2. Union of set `S` and `T`, `S|T`
3. Intersection of `S` and `T`, `S&T`
4. Check if item `x` in set `S`, `(1<<x)&S`, check if x-th bit (counting from **right to left**) is 1
5. Add item `x` to set `S`, `S = (1<<x)|S`
6. Remove item `x` from set `S`, `S = S&~(1<<x)`, set x-th bit to 0
#### Analysis
Pros: Bitmap implementation of map is fast if the size of all possible elements is smaller than word length (which usually corresponds to the integer size). Constant time $O(1)$ for all operation in this case.  
Cons: If universe of possible set elements exceeds word size, we need to distribute the universe into multiple words, add to the operation time. $O(\text{universe size}/\text{word size}+1)$. First time for word lookup, then constant time for operation within the word.  

### Set in Python - Hash Implementation
We can treat set as **hash** table with only keys.
#### Operations
1. Add item and remove item, `set.add` & `set.remove`, $O(1)$ expected time.
2. Build a set of `n` elements, equivalent to add `n` times, $O(n)$ 
3. Union of `S` and `T`, $O(n)$ as we need to traverse both sets
4. Intersection of `S` and `T`, $O(n)$ as we need to traverse at least the smaller of two sets
6. Check if item in set, $O(1)$

# Bloom Filter
Fast but inexact representation of large sets.  
Possible false positive but no false negative.  
> False positive: the item is not in the set but test report true.  

If item $x\in S$, always report true.  
If ttem $x\notin S$, may still report true$.  
```
 Gnd\Rst  True    False

 True     TP      FN

 False    FP      TN
```
## Abstract Data Type
### Data
A combination of hash an bitmap.  
A set `S` of `n` values to be represented.  
Parameter `k`, a small integer, each value are hashed to a `k`-tuple.  
Table `H` of size `N` bits, `N > kn`, larger than possible number of items time k-bit representation per item.  
### Operation
1. Initially set all bits of `H` to 0
2. To add item `x`, hash `x` get k-tuple, set all corresponding k-bits to 1
3. To check if item `x` in set, hash `x` get k-tuple, check if every k-bits are 1s
4. Union of `S` and `T`, bit-or of two sets

## Application
1. Allow access with allow-list, prevent denial of service attack with firewall.  
2. Deny access with block-list, allow most of the legitimate traffic.  

## Analysis
### Advantages  
1. Fast even on large sets  
2. Supports adds and unions
3. We don't need to know the universe of possible items because of using hash, unlike bitmap.

### Disadvantages
1. Inexact as there might be false positive reports
2. No remove and intersection support

### False Positive Rate
Fixed value number of bits representing each item `k`, bit table size `N`.  
Let `p` be the probability that one bit is 1.  
For a key `x` thats not in the set, the probability of false positive is $p^k$, which is the probability all `k` bits representing `x` is 1.  


# Cuckoo Filter
Fast but inexact representation of large sets.  
The implementatio is similar to Cuckoo Hashing.  
## Abstract Data Type
### Data
A key can be stored in one of **two locations**, based on the hash values of two hash functions $h_0$ and $h_1$.  
We could use a single table `H`, or two tables $H_0$ and $H_1$.  

### Operation 
1. Search for key `x`: look in both possible locations
2. Insert key `x`: Store fingerprint of `fp(x)` the first location, if bucket is full (already has `k` fingerprints), **randomly** choose one of the `k` and move it to its other position and so forth  
3. Delete key `x`: look for fingerprint of the key in both locations, if `fp(x)` is found in either location, remove an instance of `fp(x)`. Can cause incorrect deletion if item `x` is not previously added  


### Cuckoo Filter vs Cuckoo Hashing
Cuckoo Filter is a set implementation  
1. store only the fingerprint `fp(x)` of the key
2. each position of the table can contain `k` fingerprints, **multiple** data in each position
3. can be implemented using one table or two tables
4. fingerprint is stored we need a new technique to calculate its other position  

Cuckoo Hashing is a hash table implementation  
1. store the entire key itself in the table
2. each position of the table can store only one key
3. needs to be implemented with two tables
4. since the original key is stored, we can calculate its other position with ease

## Implementation Details
### Calculate Positions
As mentioned above, since we are storing the **fingerprints** of keys instead of the keys, we **can not** know the other position of a fingerprint `fp(x)` just by applying the other hash function.   
Solution is to exploit the reversible property of the XOR $\oplus$  
Two hash functions $h_0$ and $h_1$, fingerprint of key `k` $fp(k)$.  
1. First location $h_0(k)$, fingerprint stored $fp(k)$  
2. Second location $h_0(k)\oplus h_1(fp(k))$, data stored also $fp(k)$  
3. For a simplified solution we can dump the second hash function $h_1$ all together  
#### Calculate Position for Search 
Known original key $k$, $h_0$, $h_1$ and $fp$ when start searching.  
Location 1 = $h_0(k)$  
Location 2 = $h_0(k)\oplus h_1(fp(k))$  
#### Calculate Second Position From The First Position
During the insertion move around process, we don't know the key of the fingerprints we encountered.  
Known $h_0$, $h_1$, fingerprint $fp(k)$ stored in the first position $h_0(k)$.  
Second location = $h_0(k)\oplus h_1(fp(k))$  
#### Calculate First Position From The Second Position
Known $h_0$, $h_1$, fingerprint $fp(k)$ stored in the first position $h_0(k)\oplus h_1(fp(k))$.  
Second location = $h_0(k)\oplus h_1(fp(k)) \oplus h_1(fp(k)) = h_0(k)$.  
Conclusion XOR current location with hash_1 of stored fingerprint to get the other location.  



# Binary Search Tree

# Special Balanced Trees
## B-trees/Multiway Trees
Each node have multiple children. 
$$\text{keys} = \text{children} - 1$$
# Alternatives to Balanced Trees with Randomization
## Treaps
Tree + Heap
## skip lists
## zip trees
