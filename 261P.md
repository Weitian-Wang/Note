- [Introduction](#introduction)
  - [Focus of the course](#focus-of-the-course)
  - [Abstract Data Type vs Data Structure](#abstract-data-type-vs-data-structure)
    - [Abstract Data Type](#abstract-data-type)
    - [Data Structure](#data-structure)
    - [Examples](#examples)
- [Analysis of Data Structures](#analysis-of-data-structures)
  - [Worst-case](#worst-case)
  - [Average-case](#average-case)
  - [Amortized Analysis](#amortized-analysis)
    - [Amortized Analysis with Potential Function Method](#amortized-analysis-with-potential-function-method)
- [Array](#array)
  - [Abstract Data Type](#abstract-data-type-1)
    - [Data](#data)
    - [Operations](#operations)
- [Array List - Dynamic Arrays](#array-list---dynamic-arrays)
  - [Abstract Data Type](#abstract-data-type-2)
    - [Data](#data-1)
    - [Operations](#operations-1)
  - [Implementation](#implementation)
    - [Data](#data-2)
    - [Operation \& Amortized Analysis](#operation--amortized-analysis)
      - [Increment](#increment)
      - [Decrement](#decrement)
- [Stack](#stack)
  - [Abstract Data Type](#abstract-data-type-3)
    - [Data](#data-3)
    - [Operations](#operations-2)
  - [Implementation](#implementation-1)
- [FIFO Queue](#fifo-queue)
  - [Abstract Data Type](#abstract-data-type-4)
    - [Data](#data-4)
    - [Operations](#operations-3)
- [Deque - Double-Ended Queue](#deque---double-ended-queue)
  - [Abstract Data Type](#abstract-data-type-5)
    - [Data](#data-5)
    - [Operations](#operations-4)
- [Dictionary Problem](#dictionary-problem)
  - [Abstract Data Type](#abstract-data-type-6)
    - [Data](#data-6)
    - [Operations](#operations-5)
- [Hashing](#hashing)
  - [Basics](#basics)
    - [Load Factor](#load-factor)
    - [Hash Function](#hash-function)
      - [k-independent Hashing](#k-independent-hashing)
        - [Generate k-independent Hash Functions](#generate-k-independent-hash-functions)
        - [Usage](#usage)
      - [Tabulation Hashing](#tabulation-hashing)
        - [Generate Tabulation Hash](#generate-tabulation-hash)
        - [Implementation](#implementation-2)
    - [Rehashing](#rehashing)
  - [Hash Collision](#hash-collision)
    - [Hash Chaining](#hash-chaining)
      - [Operations](#operations-6)
      - [Advantage](#advantage)
      - [Disadvantage](#disadvantage)
    - [Linear Probing](#linear-probing)
      - [Analysis](#analysis)
      - [Code](#code)
      - [Disadvantage](#disadvantage-1)
    - [Quadratic Probing](#quadratic-probing)
    - [Double Hashing](#double-hashing)
    - [Cuckoo Hashing](#cuckoo-hashing)
      - [Visualizing Cuckoo Hashing](#visualizing-cuckoo-hashing)
        - [Directed Graph](#directed-graph)
        - [Undriected Graph](#undriected-graph)
      - [Loop Detection](#loop-detection)
      - [Analysis](#analysis-1)
- [Priority Queue](#priority-queue)
  - [Abstract Data Type](#abstract-data-type-7)
    - [Data](#data-7)
    - [Operation](#operation)
  - [Data Structure](#data-structure-1)
    - [Naive List](#naive-list)
    - [Binary Heap](#binary-heap)
      - [Operations](#operations-7)
      - [Analysis](#analysis-2)
    - [K-ary Heap](#k-ary-heap)
      - [Operations](#operations-8)
    - [Fibonacci Heap](#fibonacci-heap)
      - [Operations](#operations-9)
      - [Analysis](#analysis-3)
        - [Amortized Time of Decrease Priority](#amortized-time-of-decrease-priority)
        - [Amortized Time of Delete Min](#amortized-time-of-delete-min)
  - [Application](#application)
    - [Dijkstra's Algorithm](#dijkstras-algorithm)
      - [Dijkstra's Algorithm with Binary Heap](#dijkstras-algorithm-with-binary-heap)
      - [Dijkstra's Algorithm with K-ary Heap](#dijkstras-algorithm-with-k-ary-heap)
      - [Dijkstra's Algorithm with Fibonacci Heap](#dijkstras-algorithm-with-fibonacci-heap)
    - [Heap Sort](#heap-sort)
- [Set](#set)
  - [Abstract Data Structure](#abstract-data-structure)
    - [Data](#data-8)
    - [Operation](#operation-1)
  - [Implementation](#implementation-3)
    - [Bitmap](#bitmap)
      - [Bitmap Set Operations](#bitmap-set-operations)
      - [Analysis](#analysis-4)
    - [Set in Python - Hash Implementation](#set-in-python---hash-implementation)
      - [Operations](#operations-10)
- [Bloom Filter](#bloom-filter)
  - [Abstract Data Type](#abstract-data-type-8)
    - [Data](#data-9)
    - [Operation](#operation-2)
  - [Application](#application-1)
  - [Analysis](#analysis-5)
    - [Advantages](#advantages)
    - [Disadvantages](#disadvantages)
    - [False Positive Rate](#false-positive-rate)
- [Cuckoo Filter](#cuckoo-filter)
  - [Abstract Data Type](#abstract-data-type-9)
    - [Data](#data-10)
    - [Operation](#operation-3)
    - [Cuckoo Filter vs Cuckoo Hashing](#cuckoo-filter-vs-cuckoo-hashing)
  - [Implementation Details](#implementation-details)
    - [Calculate Positions](#calculate-positions)
      - [Calculate Position for Search](#calculate-position-for-search)
      - [Calculate Second Position From The First Position](#calculate-second-position-from-the-first-position)
      - [Calculate First Position From The Second Position](#calculate-first-position-from-the-second-position)
- [Binary Search Tree](#binary-search-tree)
  - [Introduction](#introduction-1)
  - [Binary Tree](#binary-tree)
    - [Abstract Data Type](#abstract-data-type-10)
      - [Data](#data-11)
    - [Properties](#properties)
      - [Depth](#depth)
      - [Height](#height)
      - [Traversal Orders](#traversal-orders)
      - [Number of Nodes](#number-of-nodes)
  - [Binary Search Tree](#binary-search-tree-1)
    - [Abstract Data Type](#abstract-data-type-11)
      - [Data](#data-12)
      - [Operation](#operation-4)
    - [Analysis](#analysis-6)
      - [Search Time](#search-time)
      - [Delete and Insert](#delete-and-insert)
      - [Analysis of Insertion](#analysis-of-insertion)
    - [Strategies to Minimize Height](#strategies-to-minimize-height)
      - [Rotations](#rotations)
  - [AVL Trees](#avl-trees)
    - [Abstract Data Type](#abstract-data-type-12)
    - [Operation \& Analysis](#operation--analysis)
  - [Red Black Trees](#red-black-trees)
    - [Abstract Data Type](#abstract-data-type-13)
    - [Operation \& Analysis](#operation--analysis-1)
    - [Comparison](#comparison)
  - [Rank-Balanced Trees](#rank-balanced-trees)
    - [Abstract Data Type](#abstract-data-type-14)
    - [Special Cases](#special-cases)
  - [Weak AVL (WAVL) Trees](#weak-avl-wavl-trees)
    - [Abstract Data Type](#abstract-data-type-15)
      - [Data](#data-13)
      - [Operation](#operation-5)
        - [Insertion](#insertion)
        - [Deletion](#deletion)
      - [Analysis](#analysis-7)
- [Special Balanced Trees](#special-balanced-trees)
  - [Recap \& Conclusion of Binary Search Tree](#recap--conclusion-of-binary-search-tree)
  - [Multiway Trees](#multiway-trees)
    - [B-tree or (a, b)-tree](#b-tree-or-a-b-tree)
      - [Abstract Data Type](#abstract-data-type-16)
      - [Operation](#operation-6)
        - [Search](#search)
        - [Insertion](#insertion-1)
        - [Deletion](#deletion-1)
      - [Application of B-tree](#application-of-b-tree)
  - [Alternatives to Balanced Trees with Randomization](#alternatives-to-balanced-trees-with-randomization)
  - [Alternatives that Improve Amortized Performance](#alternatives-that-improve-amortized-performance)
  - [Treaps](#treaps)
  - [skip lists](#skip-lists)
  - [zip trees](#zip-trees)


# Introduction
## Focus of the course
Anlyze the performance of the algorithm implemented with given data structure.  
## Abstract Data Type vs Data Structure
### Abstract Data Type
Abstract data type defines the **logical** form of the data type. We care about the data type and operations about ADT.
### Data Structure
The data structure implements the **physical** form of the data type.  
### Examples
Dictionary
Abstract Data Type:  
- Data type: key-value pairs  
- Operations: 
  - Query: find value associated with a given key
  - Update: store value of a given key  

Data Structure:  
The actual implementation of the ADT above:  
- Hashing schemes
- Balanced binary search trees

# Analysis of Data Structures
## Worst-case
Restrictive time used for real-time response time analysis.
## Average-case
Expected time value, taking input probilities in to account. As we are making assumption about the probilities, average-case time can be inaccurate.
## Amortized Analysis
**Worst** case time for a **sequence** of operations.  
$$\text{Total Actual Time} \leq \text{Total Amortized Time}$$
Worst-case amortized time is the upper bound of worst-case actual time. [Proof see note 2-15](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes2-handout.pdf).  
### Amortized Analysis with Potential Function Method
1. Define a non-negative potential function $\Phi$, describing the states of the data structure. Initially $\Phi=0$
> Think potential function as the distance between current state of the data structure and the idea state. One analogy would be the gravitational potential energe.  

2. Define amortized time of an operation:
$$\text{amortized time} = \text{actual time} + C\times(\Phi_{new}-\Phi_{old})$$
> The amortized time equals actual time plus change of potential. If change of potential is positive, the data structure is further from its idea state, which is going to cost more time for future operations.  

> The potential is 0 when the data structure is initialized.  
> The potential of the data structure is non-negative(zero or positive).    
> The change of potential $\Delta\Phi=\Phi_{new}-\Phi{old}$ may be negative.
> The actual time of an operation is always positive.
> The amortized time of an operation may be negative, which will make it O(1).

# Array
## Abstract Data Type
### Data
Contiguous store of data items, with fixed length
### Operations
1. Create array of length n, 0-indexed by convention
2. Store a given value at a given index
3. Retrieve value stored at a given idex


# Array List - Dynamic Arrays
## Abstract Data Type
### Data
Continuous store of data items, with variable length. Combines the functionalities of arrays and linked lists.  
### Operations
1. Create a new ArrayList of length n
2. Return length of the current ArrayList
3. Store an item in given location index i
4. Reterieve the item stored in location index i
5. Increase the current length by 1
6. Decrease the current length by 1
## Implementation
Use fixed length array to implement ArrayList
### Data
Current number of elements (or length) L.  
Underlying array B, with $|B|\geq L$.
> $|B|$ denotes the length of array B.  

Keep $|B|/4 \leq L \leq |B|$, if smaller than $|B|/4$ shrink size to idea size, if larger than $|B|$ expand size to idea size.     
Ideal state $L = |B|/2$
### Operation & Amortized Analysis
Given that the ideal state of this data structure implementation is $L = |B|/2$, we can define the potential function
$$\Phi=|2L-|B||$$
#### Increment
Increase the length of ArrayList by 1, if L is too big that it exceeds $|B|$, reallocate underlying array.
```
L = L + 1
if L > sizeof(B):
  B_NEW = allocate new array of size 2*L
  copy B in to B_NEW
  set remaining location of B_NEW to null
  B = B_NEW
```
Amortized time for increment operation where B is resized:  
$\text{Potential Fucntion} \Phi=|2L-|B||$  
$\text{Amortized Time} = \text{Actual Time} + C\times \Delta\Phi$  
$\text{Actual Time} \leq c\times(L+1)$ for copying $L+1$ elements.  
$\Phi_{old}=|2L-|L||=L$  
$\Phi_{new}=|2L-|2L||=0$  
$\text{Amortized Time} \leq c\times(L+1) + c\times(\Phi_{new}-\Phi_{old})$  
$=c\times(L+1)+c\times(-L)=c=O(1)$  
#### Decrement
Decrease the length of ArrayList by 1, if L is smaller than $|B|/4$, resize the underlying array to $2L$.  
Amortized time for decrement operation where B is resized:  
$\text{Potential Fucntion} \Phi=|2L-|B||$  
$\text{Amortized Time} = \text{Actual Time} + C\times \Delta\Phi$  
$\text{Actual Time} \leq c\times(L-1)$ for copying $L-1$ elements.  
$\Phi_{old}=|2L-|4L||=2L$  
$\Phi_{new}=|2L-|2L||=0$  
$\text{Amortized Time} \leq c\times(L+1) + c\times(\Phi_{new}-\Phi_{old})$  
$=c\times(L+1)+c\times(0-2L)=-cL=O(1)$  
> As per lecture note, negative amortized time is valid, and quantity would be $O(1)$
```
L = L - 1
if 4*L < sizeof(B):
  B_NEW = allocate new array of size 2*L
  copy data, set other locations
  B = B_NEW
```

# Stack
## Abstract Data Type
### Data
Contiguous store of data items, with variable length and FILO/LIFO properties.
### Operations
1. Create an empty stack
2. Push: insert an item at the top of the stack
3. Pop: remove the item at the top of the stack
## Implementation
Stack can be implemented with ArrayList. When using implementing something with known data structure, we can use its amortized operation times as new base operation times.

# FIFO Queue
## Abstract Data Type
### Data
Contiguous store of data items, with variable length and FIFO properties.
### Operations
1. Create an empty queue
2. Enqueue: insert an item at the rear of the queue
3. Dequeue: Remove the item at the front of the queue

# Deque - Double-Ended Queue
## Abstract Data Type
### Data
Contiguous store of data items, with combined functionality of stack and queue.
### Operations
1. Create and empty deque
2. Insert an item at the front of the deque
3. Insert an item at the rear of the deque
4. Remove an item at the front of the deque
5. Remove an item at the rear of the deque

# Dictionary Problem
## Abstract Data Type
### Data
Collection of key-value pairs.  
Keys can be numbers, string, memory address, etc.  
Values can be basic data types or references.
### Operations
1. Search value associated with given key
2. Update value of given key, or add new pair if key not previously exist in the dictionary
3. Delete pair associated with given key

# Hashing
Suppose we have n key-value pairs, n may change as we perform set/delete operations.  
Maintain a hash table H of size N > n, N may require resizing as n changes.
## Basics
### Load Factor
Load factor is the number of elements in a hash table divided by the total number of table slots.  
$$\alpha=\frac{n}{N}$$
### Hash Function
For $K$ possible keys to $N$ possible index values.  
Hash function h: $\text{keys}\rarr\text{indices in hash table}$  
#### k-independent Hashing
Definition of k-independent: for every $k$-tuple of keys, all $N^k$ $k$-tuples of indice values are equally likely.
##### Generate k-independent Hash Functions
1. Choose a prime $p\gg N$
2. Randomly choose k numbers $a_0, a_1, \dots, a_{k-1}$ in range $[0, p-1]$
3. User polynomial to generate hash value $h(x)=((a_0+a_1\cdot x+a_2\cdot x^2+\dots+a_{k-1}\cdot x^{k-1})mod\ p)mod\ N$
##### Usage
- Hash chaining: 2-independence for O(1) expected time
- Linear probing: 5-independence
- Cuckoo hashing: $O(\log n)$-independence
#### Tabulation Hashing
##### Generate Tabulation Hash
Assume keys are 32-bit integers -> 4 byte -> 4 8-bit segmentations
1. Preprocessing: Build four tables $T_i[\cdot]$ of 256 random numbers each.
2. h(k):
    - Partition k in to 4 bytes: $k_0, k_1, k_2, k_3$
    - Return $T_0(k_0)\oplus T_1(k_1)\oplus T_2(k_2)\oplus T_3(k_3)$
> $\oplus$ bitwise XOR
##### Implementation
Above tabulation hash function is 3-independent.
### Rehashing
If we need to resize H to accomodate more elements, we need choose a new hash function that map all possible key to new N indices.
## Hash Collision
Different hash key mapped to the same hash index value.  
Few strategies discussed in the lecture for dealing with hash collsions:
### Hash Chaining
Each cell of hash table H store a collection (as ArrayList or linked list) of key-value pairs.
#### Operations
Estimated time per operation = $O(1+\alpha)$.
> 1 comes from the inevitable hash opertion and collection lookup  
> The number of n-1 existing keys colliding with k is $(n-1)\times\frac{1}{N}$, assuming hash function generate N indices with the same likelihood.  
1. Search(key): first find the collection in the hash table, then scan through the collection, looking for the pair corresponds to given key.
2. Set(key, value): Update pair or add new pair to the collection.
3. Delete(key): delete the pair in the collection.
#### Advantage
Hash chaining is a simple solution that works.  
#### Disadvantage
Extra space for storage and slower access time.
### Linear Probing
Each cell of hash table H store one key-value pair.  
Try to store (k,v) in index position h(k), if full try h(k)+1, h(k)+2, so forth and wrap around modulo N.
#### Analysis
Omit the proof, the expect search time of hash table with linear probing is $O(1)$.  
Expected time for successful search
$$O\left(1+\frac{1}{(1-\alpha)}\right)$$
Expected time for unsuccessful search
$$O\left(1+\frac{1}{(1-\alpha)^2}\right)$$
#### Code
```
def search(k):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  // key == k
  if H[i] is non-empty:
    return H[i].value
  else:
    exception
```
```
def set(k,v):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  H[i] = v
```
A simple solution for deletion would be mark the deleted position with a flag, indicating the value is nolonger available.  
The following code is for moving probed elements to the front.
```
def delete(k):
  i = h(k)
  while H[i] is non-empty and key != k:
    i = (i+1)%N
  if H[i] is empty:
    exception
  j = (i+1)%N
  while H[j] is non-empty:
    if h(H[j].key) not in circular range [i+1, j]:
      move H[j] to H[i]
      i = j
    j = j + 1
```
i points to available empty spot in the left side  
j scan through current chunck of elements      
#### Disadvantage
Performance degrades as load factor $\alpha=\frac{n}{N}$ gets close to 1.
### Quadratic Probing
Instead of looking for next one slot, look for $h(k)+1$, $h(k)+4$, $h(k)+9$, etc.
### Double Hashing
User a secondary hash function $h_2(k)$, try $h(k)+h_2(k)$, $h(k)+2\times h_2(k)$, $h(k)+3\times h_2(k)$, etc. 
### Cuckoo Hashing
Two hash tables: $H_0$, $H_1$  
Two hash functions: $h_0$, $h_1$  
Search(k): Look in both places $H_0[h_0(k)]$, $H_1[h_1(k)]$  
Set(k,v): Start from table zero, insert at $H_0[h_0(k)]$, if pair $(k', v')$ is already there, evict it to $H_1[h_1(k')]$ and so forth.  
#### Visualizing Cuckoo Hashing
We visualize cells as vertices, keys as edges. 
##### Directed Graph
Direct each edge **towards** the cell that cintains the key.  
A graph is a valid state only if each vertex (cell) has at most 1 incoming edge.
##### Undriected Graph
If two vertices have three **paths**, there is no way to orient the edges so that each vertex has only one incoming edge.
#### Loop Detection
1. Loop time threshold: project 1 implementation
2. Explicitly check for cycle: homework 1 track the starting key, if we try to place it three times ($t_0$, $t_1$, $t_0$ again) we are in the loop.
#### Analysis
Guranteed $O(1)$ worst-case search time, at the cost of slower set operation.  
For a sequence of n operations, the expect number of rebuild is 1, rebuild takes $O(n)$, so expect time **per operation** is O(1).

# Priority Queue
## Abstract Data Type
API
### Data
A set of items with associated priorities. By convention the smaller numbers have the higher priorities.
### Operation
1. Create priority queue for a set of items
2. Add and remove items
3. Find the item with minimum value
4. Change the priority of an item
## Data Structure
Physical implementations of a priority queue can be lists of heaps.
### Naive List
Use unsorted lists or sorted lists to implement priority queues.  
Homework 2 represent the queue as a dynamic array of data values.  
### Binary Heap
An **array** $A$ of $n$ values interpreted as a complete binary tree.  
1. Root = $A[0]$  
2. Children($A[i]$)=$(A[2i+1],\ A[2i+2])$  
3. Parent($A[i]$)=$A[floor\left ( \frac{i-1}{2}\right )]$  
4. Height = $\log_2 (n)$
#### Operations
1. Find min: return $A[0]$
2. Sift down at index i: if $A[i]$ is larger than its descendants, move it down the tree. Worst-case $O(\log n)$.  
    ```
    def siftDown(i):
        // two comparison per iteration
        while A[i] larger than any of two children:
            swap A[i] with best child
            // update i continue sift down until smaller than both children
            i = index of best child
    ```
3. Sift up at index i: if $A[i]$ is smaller than its parent, move it up the tree. Worst-case $O(\log n)$.  
    ```
    def siftUp(i):
        // one comparison per iteration 
        while A[i] smaller than parent:
            swap A[i] with parent
            // update i continue sift up until ;arger than parent
            i = index of best parent
    ```
4. Delete item at given index $i$: move last element $A[n-1]$ to $A[i]$, then do both sift_up(i) and sift_down(i)
5. Delete min would be delete item at index 0
6. Make heap from an array A of n items
    ```
    def makeHeap():
        for i in n-1, n-2, ..., 1, 0:
            siftDown(i)
    ```
    > Makeheap analysis see [note 5-11](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes5-handout.pdf).  
    > Why we run siftDown on each position for creation rather than siftUp? Operation siftDown is costy for leaves heigher up in the tree, siftUp is costy for leaves closer to the bottom. There are more nodes closer to the bottom in a complete binary tree. Therefore siftDown is the faster choice.


#### Analysis
Make heap from a set of n items $O(n)$ time, all other operations have $O(\log n)$ time.
### K-ary Heap
Instead of two children, each node in a k-ary heap have k children.  
Suppose we have k-ary tree with n nodes.  
Height of tree: $\log_k n = \frac{\log k}{\log n}$  
Children($A[i]$) = $(A[ki+1],\ A[ki+2],\ A[ki+3], \dots, A[ki+k])$  
Parent($A[i]$) = $A[floor\left ( \frac{i-1}{k}\right )]$  
#### Operations
1. Sift up is the same as binary heap, do 1 comparison per level
2. Sift down: we need to find the minimum amongst all k children. Which makes it $O(k\cdot\text{height})$
### Fibonacci Heap
**Forest** of heap-ordered trees with pointer to the best tree root.  
Each node has:  
1. Data item and priority value
2. Pointer to parent node
3. Number of children
4. Pointer to a child node
5. Doubly-link pointers to left and right neighbors.
6. A Boolean flag, True when
    - node is not root
    - has one child removed  

Potential function $\Phi = (\text{number of root nodes} + 2 \cdot(\text{number of True flags}))$  

#### Operations
1. Find min: return item pointed be the min pointer, which points to the minimum value amongst all root nodes. $\Delta\Phi=0$.
2. Create new Fibonacci Heap from n items: create a root node for each item, link items. $\Delta\Phi=n$ as we created n more root nodes.
3. Insert item: create root node for the new item, update min pointer. $\Delta\Phi=1$ as we created 1 more root node.
4. Merge **two** Fibonacci Heaps: Concatenate root lists and update best root pointer. $\Delta\Phi=0$.  
> We are doing operation in the laziest fashion so far. The strategy is to delay the work until we can no longer ignore them.  

5. Decrease priority of target node x:
    - If x is root, decrease priority inplace
    - If x is not a root, make it x a root then decrease
        - If x's parent p is not root, and flag is False, set to True
        - If x's parent p is not root, and flag is True, make p a root, and so forth for p's parent.
    > Remove subtree from one tree may cause a chain reaction of node updates or promotions (become root node).  
  
    > Meaning of the boolean flag is to track when a non-root node loses its second child, it has to become a root.

6. Delete minimum: delete the root where minimum value resides, children of deleted root become roots and set the flags as False. Then improve the forest.
7. Improve: force all **root nodes** to have different numbers of children (at most $M=\log n$).
      ```
      // use array C to track roots' numbers of children
      // initially C is empty and R contains all roots
      // allocate array C of size M+1, let R be the collection not yet in C
      while R is nonempty:
          remove x from R
          // no children number conflict
          if C[x.childCount] == null:
              C[x.childCount] = x
          else:
              // set x as children of existing root
              combine x, C[x.childCount] to a new tree y
              // add new tree to working collection
              add y to R
              // remove existing root with same number of children as x
              C[x.childCount] == null
      ```
#### Analysis
##### Amortized Time of Decrease Priority
Let k be the number of promotions triggered by one decrease priority operation.  
Actual time: $O(k+1)$ for k promotions and decrease priority.  
With k promotions we created k new roots.   
With k promotions we also flipped k-1 (or k if taget node has True flag) flags from True to False.  
Change of potential $\leq k-2(k-1)+2=-k+O(1)$. Worst case amortized time = $O(1)$.
##### Amortized Time of Delete Min
Overall amortized time $O(M)$, where $M$ is the maximum number of children that a node is allowed to have. We choose $M=\log n$ to get $O(\log n)$ as amortized time for DeleteMin.
## Application
### Dijkstra's Algorithm
The given graph contains n vertices and m edges.  
Initialize a priority queue Q of vertices.  
```
// iterate n times
while Q is nonempty:
   find and remove minimum vertex v
   // iterate m times in total, not in one loop
   for each edge from v -> w:
      // update priority
      D[w] = min(D[w], D[v] + length(v->w))
```
#### Dijkstra's Algorithm with Binary Heap
Delete minimum is executed n times $O(n\log n)$.  
Updata priority is executed m time $O(m\log n)$.  
> Use additional dictionary to track the nodes so we don't need to search for the nodes in the array before changing priorities.  
  
> As for Dijkstra's algo we are only decreasing the priorities of nodes, therefore **sift up** would be sufficient to fix heap after change priorities or delete min.  

Total runtime of Dijkstra's algorithm with binary heap $O(n\log n + m\log n)$. For a connected graph m >= n - 1, which overshadows n. Hense time $O(m\log n)$. EXPLANATION NOT VARIFIED  

#### Dijkstra's Algorithm with K-ary Heap
Recap the level of k-ary tree with n nodes = $\log_k n = \frac{\log k}{\log n}$  

Delete min operation now cost $O(k\cdot\frac{\log n}{\log k})=O(\frac{k}{\log k}\cdot\log n)$, which is **slower** than binary heap's $O(\log n)$.  

Decrease priority operation cost one conparison each level, but with k-ary tree there are fewer levels. $O(1\cdot \text{depth of tree}) = O(\frac{1}{\log k}\cdot\log n)$, which is **faster** than than binary heap's $O(\log n)$.  

The total time $O(n\cdot\text{deleteMin}+m\cdot\text{decreasePriority})$ = $O(n\cdot\frac{k}{\log k}\cdot \log n + m\cdot\frac{1}{\log k}\cdot \log n)$.  

Proof ommited. Choose $k=\frac{m}{n}$ and when $m=n^c,\ c > 1$, the complexity simplifies to $O(m)$.

> Interpretation: for graph with number of edges that is exponential to the number of vertices, Dijkstra's algo with k-ary heap runs at $O(m)$ time.

#### Dijkstra's Algorithm with Fibonacci Heap
Recap decrease priority $O(1)$ and delete min $O(\log n)$.  
Total time for Dijkstra's algorithm with Fibonaccia Heap = $O(m+n\log n)$
### Heap Sort
Implement priority queue as a heap for a selection sort algorithm. 

# Set
## Abstract Data Structure
### Data
Each item is included only once without repetitions. For Python, only hashable items are allowed to be stored in a set.  
### Operation
1. Construct a set from a list of items
2. Union of two sets
3. Intersection of two sets
4. Compare two sets
5. Check if item in set
6. Add item to set
7. Remove item from set

## Implementation
### Bitmap
When the elements are non-negative integers `[0, 1, 2, ..., n-1]` total of `n` elements or can be numbered using small integers. We can use n-bits to represent the n-elements.  
The n-bits binary vector can be interpreted as a integer number.  
For example the set of `{0, 3, 6}` where elements `x = 6`, `y = 3` and `z = 0`, can be represented as following binary vector.  
```
bit  7 6 5 4 3 2 1 0

val  0 1 0 0 1 0 0 1
       ▲     ▲     ▲
ele    x     y     z
``` 
Integer value of the binary vector = $2^x + 2^y + 2^z$.
#### Bitmap Set Operations
1. Create `{x, y, z}`, `(1<<x)|(1<<y)|(1<<z)`, left shift 1 bit multiple times according to the element value, then bit-or the results.
2. Union of set `S` and `T`, `S|T`
3. Intersection of `S` and `T`, `S&T`
4. Check if item `x` in set `S`, `(1<<x)&S`, check if x-th bit (counting from **right to left**) is 1
5. Add item `x` to set `S`, `S = (1<<x)|S`
6. Remove item `x` from set `S`, `S = S&~(1<<x)`, set x-th bit to 0
#### Analysis
Pros: Bitmap implementation of map is fast if the size of all possible elements is smaller than word length (which usually corresponds to the integer size). Constant time $O(1)$ for all operation in this case.  
Cons: If universe of possible set elements exceeds word size, we need to distribute the universe into multiple words, add to the operation time. $O(\text{universe size}/\text{word size}+1)$. First time for word lookup, then constant time for operation within the word.  

### Set in Python - Hash Implementation
We can treat set as **hash** table with only keys.
#### Operations
1. Add item and remove item, `set.add` & `set.remove`, $O(1)$ expected time.
2. Build a set of `n` elements, equivalent to add `n` times, $O(n)$ 
3. Union of `S` and `T`, $O(n)$ as we need to traverse both sets
4. Intersection of `S` and `T`, $O(n)$ as we need to traverse at least the smaller of two sets
6. Check if item in set, $O(1)$

# Bloom Filter
Fast but inexact representation of large sets.  
Possible false positive but no false negative.  
> False positive: the item is not in the set but test report true.  

If item $x\in S$, always report true.  
If ttem $x\notin S$, may still report true$.  
```
 Gnd\Rst  True    False

 True     TP      FN

 False    FP      TN
```
## Abstract Data Type
### Data
A combination of hash an bitmap.  
A set `S` of `n` values to be represented.  
Parameter `k`, a small integer, each value are hashed to a `k`-tuple.  
Table `H` of size `N` bits, `N > kn`, larger than possible number of items time k-bit representation per item.  
### Operation
1. Initially set all bits of `H` to 0
2. To add item `x`, hash `x` get k-tuple, set all corresponding k-bits to 1
3. To check if item `x` in set, hash `x` get k-tuple, check if every k-bits are 1s
4. Union of `S` and `T`, bit-or of two sets

## Application
1. Allow access with allow-list, prevent denial of service attack with firewall.  
2. Deny access with block-list, allow most of the legitimate traffic.  

## Analysis
### Advantages  
1. Fast even on large sets  
2. Supports adds and unions
3. We don't need to know the universe of possible items because of using hash, unlike bitmap.

### Disadvantages
1. Inexact as there might be false positive reports
2. No remove and intersection support

### False Positive Rate
Fixed value number of bits representing each item `k`, bit table size `N`.  
Let `p` be the probability that one bit is 1.  
For a key `x` thats not in the set, the probability of false positive is $p^k$, which is the probability all `k` bits representing `x` is 1.  


# Cuckoo Filter
Fast but inexact representation of large sets.  
The implementatio is similar to Cuckoo Hashing.  
## Abstract Data Type
### Data
A key can be stored in one of **two locations**, based on the hash values of two hash functions $h_0$ and $h_1$.  
We could use a single table `H`, or two tables $H_0$ and $H_1$.  

### Operation 
1. Search for key `x`: look in both possible locations, may have false positive results  
2. Insert key `x`: Store fingerprint of `fp(x)` the first location, if bucket is full (already has `k` fingerprints), **randomly** choose one of the `k` and move it to its other position and so forth  
3. Delete key `x`: look for fingerprint of the key in both locations, if `fp(x)` is found in either location, remove an instance of `fp(x)`. Can cause incorrect deletion if item `x` is not previously added  


### Cuckoo Filter vs Cuckoo Hashing
Cuckoo Filter is a set implementation  
1. store only the fingerprint `fp(x)` of the key
2. each position of the table can contain `k` fingerprints, **multiple** data in each position
3. can be implemented using one table or two tables
4. fingerprint is stored we need a new technique to calculate its other position  

Cuckoo Hashing is a hash table implementation  
1. store the entire key itself in the table
2. each position of the table can store only one key
3. needs to be implemented with two tables
4. since the original key is stored, we can calculate its other position with ease

## Implementation Details
### Calculate Positions
As mentioned above, since we are storing the **fingerprints** of keys instead of the keys, we **can not** know the other position of a fingerprint `fp(x)` just by applying the other hash function.   
Solution is to exploit the reversible property of the XOR $\oplus$  
Two hash functions $h_0$ and $h_1$, fingerprint of key `k` $fp(k)$.  
1. First location $h_0(k)$, fingerprint stored $fp(k)$  
2. Second location $h_0(k)\oplus h_1(fp(k))$, data stored also $fp(k)$  
3. For a simplified solution we can dump the second hash function $h_1$ all together  
#### Calculate Position for Search 
Known original key $k$, $h_0$, $h_1$ and $fp$ when start searching.  
Location 1 = $h_0(k)$  
Location 2 = $h_0(k)\oplus h_1(fp(k))$  
#### Calculate Second Position From The First Position
During the insertion move around process, we don't know the key of the fingerprints we encountered.  
Known $h_0$, $h_1$, fingerprint $fp(k)$ stored in the first position $h_0(k)$.  
Second location = $h_0(k)\oplus h_1(fp(k))$  
#### Calculate First Position From The Second Position
Known $h_0$, $h_1$, fingerprint $fp(k)$ stored in the first position $h_0(k)\oplus h_1(fp(k))$.  
Second location = $h_0(k)\oplus h_1(fp(k)) \oplus h_1(fp(k)) = h_0(k)$.  
Conclusion XOR current location with hash_1 of stored fingerprint to get the other location.  



# Binary Search Tree
## Introduction 
One optimal of comparison based searching algos for a **sorted** array is binary search. Find the mid point of the current search subarray and eliminate half of all candidates at a time.  
Worst-case number of comparisons is $\log_2n+1$.  
Although search in hash table can be as fast as $O(1)$, but hash table can't be used for predecessor and successor searches.  
```python
def search(nums, target):
    low, high = 0, len(nums)-1
    while low <= high:
        mid = (low+high)//2
        if target < nums[mid]:
            high = mid - 1
        elif target > nums[mid]:
            low = mid + 1
        else:
            return mid
    # target not in nums
    return -1
    # as low and high cross each other
    return high <- predecessor
    return low <- successor
```
Maintaining a sorted array as we insert and delete is expensive. Therefore although search is fast on sorted array, we need a better data structure that also support fast updates. Spoiler they are trees.   
## Binary Tree
### Abstract Data Type
#### Data
A tree with designated root node, the one with out parent.  
Each tree node has a left child and a right child.  
Tree nodes with out children are leaf nodes.  
> For extended binary trees, we treat the children of leaf nodes as special **extended nodes** rather than null pointers.  

### Properties
> Important: Please be consistent with the lecture nodes. Depth and height all start from 0.  
#### Depth
The depth of **root node** is **0**.  
The depth or level of a node is its distance from the root node.  
The depth of a binary tree is the max level of all its nodes.  
#### Height
The height of **leaf node** is **0**.  
The height of one given node is one more than the maximum height of its non-null child.  
The height of a binary tree it the height of its root node.  
height = max(left_child_height, right_child_height) + 1, which is a recursive definition.
#### Traversal Orders
Recursive definition and recursive implementation.  
The description of order refers to the order of root with respect to its children.   
1. Preorder: root, left subtree, right subtree
2. Inorder: left subtree, root, right subtree
3. Postorder: left subtree, right subtree, root
#### Number of Nodes
Max number of nodes at level $l$ is $2^l$.  
Total number of node in the tree with height $h$ is the sum of all levels $2^{h+1}-1$.  
> Note that depth and height starts from 0  
> ```
>  level  height
>    0      3              5
>                        /   \
>    1      2          2       8
>                     / \     /
>    2      1        1   4   7
>                       /
>    3      0          3
> ``` 
## Binary Search Tree
### Abstract Data Type
#### Data
Inorder search visits the nodes in ascending order.  
Binary search implicitly represents searching in a BST.  
Each node of the binary search tree contains:  
1. Data value
2. Pointers to objects representing left child and right child
3. Extra information for balancing the tree
#### Operation
1. Search for a target: If target == node value, return result, search left subtree for smaller values and right subtree for larger values.  
2. Insert a value: search for a value until reaches null pointer, insert there
3. Delete a value: search for the value, assume find value in node `v`
    - If `v` is leaf node, delete it
    - If `v` has one null child, replace it with its non-null child
    - If `v` has two child, copy data value of its predecessor or successor then delete predecessor or successor  
> The predecessor of one node is the right most child on its left subtree.  
> The successor of one node is the left most child on its right subtree.  

### Analysis
#### Search Time
At search iteration, we search one node then move on to next level until we find the target or exaust the nodes.  
Worst-case time for a search is $h+1$ where h is the height of the tree.  
> Note that height starts from 0, the height of a leaf node is 0.  
> ```
>  level  height
>    0      3              5
>                        /   \
>    1      2          2       8
>                     / \     /
>    2      1        1   4   7
>                       /
>    3      0          3
> ``` 
In a binary tree with `n` total number of nodes is small than maximum possible number of nodes in a binary tree of height `h`  
$$n \leq 2^{h+1}-1$$
$$\rArr h+1 \geq \log_2n$$
Therefore the worst case time is $h+1$ which is at least $O(\log_2n)$.  
To ensure the efficiency of binary search tree, we need to minimize its height.  
#### Delete and Insert
As seaching in BST is involved in deletion and insertion. The time complexity is the same.  
#### Analysis of Insertion
Worst-case we insert `n` elements in ascending or descending order, the tree would become a single sided list of height `n`.  
If we insert `n` elements in random order, all out come of trees are not equally likely.  
The average insertion cost is $O(\log n)$ if insertions are random. [Proof: Lecture Node 8-23 to 8-24](https://www.ics.uci.edu/~dillenco/compsci261p/notes/notes8-handout.pdf)

### Strategies to Minimize Height
Rebuild and rotation are two strategies used to balance binary trees. The lecture stresses rotation based methods. There are three tree implementations that utilize rotation to maintain tree height.  
1. AVL Trees
2. Red Black Trees
3. Rank-Balanced Trees, which covers WAVL, AVL and Red Black

> You should know the analysis of AVL trees and red-black trees:  
> 1. asymptotic worst-case cost of insertion
> 2. asymptotic worst-case cost and number of rotations on insertion
> 3. asymptotic worst-case cost and number of rotations on deletion


#### Rotations
Right rotation at node `B`, left child `A` becomes its parent, `B` becomes `A`'s right child.  
Note that `x, y, z` are subtrees.  
```
    B             A
   / \           / \
  A   ▲    =>   ▲   B
 / \  z         x  / \
▲   ▲             ▲   ▲
x   y             y   z
```
Left rotation is the inverse of right rotation.  
```
  A                 B  
 / \               / \ 
▲   B      =>     A   ▲
x  / \           / \  z
  ▲   ▲         ▲   ▲ 
  y   z         x   y
```
## AVL Trees
### Abstract Data Type
At each node of AVL trees, the height difference of the left subtree and the right subtree is either 0 or 1. By this rule a AVL's height $h = O(\log (n))$, where `n` is the total number of tree nodes.  
Aside from the data, AVL tree node stores additional height information for balancing the tree.  
### Operation & Analysis
1. Insertion: First insert node as in regular binary search tree, insert new node as leaf node, then update its ancestors' height information, rebalance unbalanced nodes if needed. Two rotations are needed at most to rebalance the AVL tree.    
2. Deletion: Delete following the same principle as binary search tree, then do as many as $O(\log n)$ rotations to rebalance the AVL tree.  

## Red Black Trees
### Abstract Data Type
Rules of Red-Black Trees
1. Red-black tree is a type of extended binary tree.  
2. All nodes are colored either red or black.  
3. All external nodes (children of leaf nodes if we don't use null pointer) are colored black.  
4. The children of red nodes must be colored black.  
5. The path form every external nodes to the tree root must contain same number of black nodes.  
6. The black height of a node `x` is the number of black nodes from `x` to a external node (not matter which one since they are the same distance), not counting `x` itself.  
### Operation & Analysis
1. Insertion: Standard insertion as BST, new node replaces a external node, color new node red and assign two external nodes to it. To fix rule 3, recolor at most $O(\log n)$ nodes. To fix rule 4, rotate at most $2$ times.  
2. Deletion: Follows the same principle as standard BST, requires at most $O(1)$ rotations.  

### Comparison
```
┌──────────┬───────────────────────┬────────────────────────────┐
│          │       AVL             │    Red-Black               │
├──────────┼──────────┬────────────┼──────────┬─────────────────┤
│          │ Rotation │ Extra      │ Rotation │ Extra           │
├──────────┼──────────┼────────────┼──────────┼─────────────────┤
│  Insert  │ 2        │ N/A        │ 2        │ O(logN) Recolor │
├──────────┼──────────┼────────────┼──────────┼─────────────────┤
│  Delete  │ O(logN)  │ N/A        │ O(1)     │ N/A             │
└──────────┴──────────┴────────────┴──────────┴─────────────────┘
```

## Rank-Balanced Trees
### Abstract Data Type
Rank-balanced tree is a abstract generalization of AVL trees. Each node has a rank, which can be height but not necessarily.  
Rank difference of a node is $\text{rank(parent(node))}-\text{rank(node)}$, rank of its parent's minus its rank.  
For the rebalancing operation:  
1. Follow up to the root and adjust ranks
2. Rotate if needed
### Special Cases
1. With a specific set of constraints, we can achieve AVL
2. With a specific set of constraints, we can achieve Red-Black
3. Weak AVL (WAVL) Trees

## Weak AVL (WAVL) Trees
> From list of topics:  
> 1. Understant the basic properties that characterize WAVL trees  
> 2. In insertion and deletion, which node are we interested in 
> 3. Given pseudocode and apply the basic WAVL rules to adjust rank

### Abstract Data Type
#### Data
Each node has a rank related to its subtree height.  
As defined in rank-balanced trees, the **rank difference** of **a node** = $\text{rank(parent(node))}-\text{rank(node)}$
> The confusing term "Rank Difference" is the property of a single tree node, its the difference of rank between its parent and itself.  

Type of node:  
1. (1,1)-node, if its two children both have rd of 1
2. (2,2)-node, if its two children both have rd of 2
3. (1,2)-node, if one of its child has rd 1 and the other one has rd 2 
> To make things more confusing the type of nodes is about the properties of the nodes' children.  

Properties of WAVL Trees:  
1. Rank difference of any nodes are either 1 or 2.  
2. Every external node has rank 0.  
3. An internal node with two external children must be a (1,1)-node.  
> Don't confuse rank and rank difference. 
> Rank be marked in the node and rank difference can be marked on the edge to the node's parent.  

Example for understanding the properties:  
From bottom up, all external nodes represented by rectangles have rank 0 by definition.  
The node 3 is a internal node with two external children, it has to be a (1,1)-node, which means its two children both have rank difference of 1. Therefore the rank of node 3 has to be 1.  
The rank of node 3 is 1 and the rank of its external sibling is 0. The rank difference can either be 1 or 2 according to the restriction. Therefore the rank of node 5 can only be 2 to satisfy the properties.  
```
     5   
   /   \
  3     □
 / \
□   □
```
#### Operation
##### Insertion
> According to the list of topics, memerizing step 1-3 is enough.  

To insert a value into the WAVL tree, assuming there are no duplicates.  
1. Search value, arrive at external node.  
2. Replace the external node with a new node `x` containing the value
3. Give `x` two external nodes, and rank of `x` would be 1 by definition
4. If tree was previously empty, `x` is the new root, end of insertion
5. Else `p` is the parent of `x`  
6. If `p` has rank 2, condition satified, end of insertion
    ```
         p
       /   \
      q     □
     / \    ▲
    □   □   x

    Successful insertion, same situation if the tree is mirrored.        
    ```
7. Else a violation will occur, as rank difference of `x` is 0 after insertion, fix needed.     
    ```
         p
       /   \
      □     □
            ▲
            x

    A rd-0 violation, fixing involves two cases depending on the rd of x's sibling s.
    If rd of s is 1, promote p
    IF rd of s is 2, rotation, two more subcases
    ```
##### Deletion
To delete a value from the WAVL tree  
1. Perform a standard BST deletion, the successor/predecessor fashion
2. Let `v` be the node that is actually deleted, the successor or predecessor, this is the actual node of interest
4. `p` is the parent of `v`
5. `s` is the sibling of `v`
6. `v` has at least one external node as its child, say `u`
7. let `x` be the other child of `v`, deletion may cause rd of `x` to become 3, rd-3 violation
8. If `x` is a internal node
    ``` 
            p
          /   \
         v     s     v is the node that actually got deleted
       /   \
      □     x
      ▲     
      u     

    The only possible rank arrangement is
        Rank(v) = 2
        Rank(u) = 0
        Rank(x) = 1
        If v is root p is null, otherwise Rank(p) = 3 or 4
    ```
9. If `x` is a external node 
    ``` 
            p
          /   \
         v     s    v is the node that actually got deleted
       /   \
      □     □
      ▲     ▲
      u     x

    The only possible rank arrangement is
        Rank(v) = 1
        Rank(u) = 0
        Rank(x) = 0
        If v is root p is null, otherwise Rank(p) = 2 or 3
    ```
TF is this nightmare, I'm not going any deeper.  

#### Analysis
The cost of fixing violations after insertion and deletion is $O(\log n)$ with $O(1)$ rotations.  

# Special Balanced Trees
## Recap & Conclusion of Binary Search Tree
Each node contains one key.  
Gurantee $O(\log n)$ **worst-case** time on search, insertion and deletion.  
Operations can be complicated if we need to maintain the BTS.  

Alternative approaches for tree structured search data structures can have following properties:  
1. Vaiable number of keys pre tree node
2. Randomization strategies for improve average case performance
3. Amortized alternatives, improve amortized performance

## Multiway Trees
In a multiway tree, each node have multiple keys and multiple children. 
$$\text{keys} = \text{children} - 1$$  
Binary tree is a special case where each node have 1 key and 2 children.  
```
            ┌───┬───┬───┬───┬───┐
            │k_1│k_2│k_3│...│k_r│
            ├───┼───┼───┼───┼───┤
            │   │   │   │   │   │
           T_0 T_1 T_2 ... ... T_r

Total number of r keys and r+1 children per node
In lecture note key k is 1-indexed & children T is 0-indexed
```
### B-tree or (a, b)-tree
#### Abstract Data Type
(a, b)-tree is a multiway-tree with restrictions on the maximum and minimum number of **children**.  
A (a, b)-tree is also called a B-tree of order `b`.  
> Important not to confuse: The parameter `a` and `b` refers to the number of **children**!  

`a` and `b` are positive integers, `b = 2a - 1`.  
The root has at least 2 children, in other word the root has at least 1 key.  
Each the node has a minimum of `a` children (except root) and a maximum of `b` children inclusive.  
$$\text{Number of children} \in [a, b]$$  
Which means each node has a minimum of `a-1` keys and a maximum of `b-1` keys inclusive.  
$$\text{Number of keys} \in [a-1, b-1]$$  
#### Operation
##### Search
Use predecessor search within the node. Find the potential child node that may contain the target. Until find node or reach null leaf node.  
##### Insertion
Insert key into a node (block as denoted by the lecture node) may cause it to overflow.  
A node should have `[a, b]` number of children and `[a-1, b-1]` keys.  
An overflow will cause a node/block to have `b` keys, `b = 2a - 1 = 2(a-1) + 1`.  
In this case split the overflown block into 2 blocks of `a-1` keys and push the 1 leftover key (the median) into the parent block as the new dividing key.  
> Insertion may cause a series of overflows and splits towards the root, handle recursively.  

##### Deletion
If key `k` to be deleted in not in leaf node, replace it with its immediate predecessor or successor then delete its predecessor or successor. This ensures that actual deletion always happen in leaf node.  
Delete key from a node may cause it to underflow.  
A node should have `[a, b]` number of children and `[a-1, b-1]` keys.  
An underflow will cause a node/block to have `a-2` keys.  
Try to get one key from one of its neighbours to refill to `a-1` keys. Two options where to look.   
1. The right most key from its left neighbour
2. The the left most key from its right neighbour
3. Push neighbour to parent as the new discriminating key to divide two siblings, move the original divider key to the underflowing block  
> The new borrowed key is not actually from its neighbour, its from the node's parent.  

If the only neighbour has a minimum number `a-1` keys, borrow one key will cause neighbour to also underflow.  
In the case above, we merge two neighbours with `a-1` keys and `a-2` keys, then we borrow the discriminating key from their parent. In total the new node will have `(a-1) + (a-2) + 1 = (2a-1) - 1 = b - 1` keys, which satifies the maximum amount of keys.  

> Insertion may cause a series of merge.  

#### Application of B-tree
## Alternatives to Balanced Trees with Randomization
## Alternatives that Improve Amortized Performance
## Treaps
Tree + Heap
## skip lists
## zip trees
